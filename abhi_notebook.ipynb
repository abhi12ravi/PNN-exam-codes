{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "abhi-notebook",
      "provenance": [],
      "collapsed_sections": [
        "sYr-p8Bry_sG",
        "VJ4An_euzSEl",
        "NVu6ik_J09OJ",
        "2Ac2N6hHlrS4",
        "JLQlnkGtBZMU"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYr-p8Bry_sG"
      },
      "source": [
        "##Library and utils\n",
        "*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Sp35f_ax-Ug"
      },
      "source": [
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score\n",
        "from math import copysign \n",
        "sign = lambda x : copysign(1, x) \n",
        "\n",
        "np.set_printoptions(precision=4)\n",
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "def transformToList(inputString):\n",
        "    s = inputString.translate(str.maketrans({'{': '[', '}': ']', '(': '[', ')': ']'}))\n",
        "    s = s.replace('[', ' [ ')\n",
        "    s = s.replace(']', ' ] ')\n",
        "    s = s.replace(',', ' , ')\n",
        "    words = s.split()\n",
        "    output = \"\"\n",
        "    for word in words:\n",
        "        if word==\"[\" or word==\"]\" or word==\",\": output+=word\n",
        "        else: output+='\"'+word+'\"'\n",
        "    out = json.loads(output)\n",
        "    try:\n",
        "        a = np.array(out).astype(np.float).tolist()\n",
        "    except:\n",
        "        a = out\n",
        "    return a\n",
        "\n",
        "class Activation:\n",
        "    def stepFunction(u):\n",
        "        if u>0: return 1\n",
        "        elif u<0: return 0\n",
        "        else: return 0.5\n",
        "\n",
        "\n",
        "class Utility:\n",
        "    def augmentArray(a, value=1, position=0):\n",
        "        return np.insert(a, position, value, axis=len(a.shape)-1)\n",
        "    \n",
        "    def sampleNorm(data, target, mainClass = 1):\n",
        "        if len(data) != len(target):\n",
        "            print(\"incompatible array sizes - sampleNorm\")\n",
        "            return\n",
        "        for i in range(len(target)):\n",
        "            if target[i] != mainClass:\n",
        "                data[i] = [-x for x in data[i]]\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ4An_euzSEl"
      },
      "source": [
        "## Week1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DX9m1ZHhzg8N"
      },
      "source": [
        "### Confusion Matrix and Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr_gzT2Mzhvl"
      },
      "source": [
        "class Week1:\n",
        "    def basic_metrics(y_true, y_pred, class_names, normalize=False):\n",
        "        cm = confusion_matrix(y_true, y_pred)\n",
        "        cm = cm[:,::-1][::-1]\n",
        "        np.set_printoptions(precision=4)\n",
        "\n",
        "        title='Confusion matrix'\n",
        "        cmap=plt.cm.Blues\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "        tick_marks = np.arange(len(class_names))\n",
        "        plt.xticks(tick_marks, class_names, rotation=45)\n",
        "        plt.yticks(tick_marks, class_names)\n",
        "\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        thresh = cm.max() / 2.\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            plt.text(j, i, cm[i, j],\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label')\n",
        "\n",
        "        print(classification_report(y_true, y_pred, target_names=class_names[::-1],digits=4))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "ZxKOObyRzxLL",
        "outputId": "ad92cecb-7262-4069-96eb-2b420d3cd346"
      },
      "source": [
        "# Confusion Matrix\n",
        "class_names = ['One', 'Zero']\n",
        "y_true = [1,1,0,1,0,1,1]\n",
        "y_pred = [1,0,1,1,0,1,0]\n",
        "Week1.basic_metrics(y_true, y_pred, class_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Zero     0.3333    0.5000    0.4000         2\n",
            "         One     0.7500    0.6000    0.6667         5\n",
            "\n",
            "    accuracy                         0.5714         7\n",
            "   macro avg     0.5417    0.5500    0.5333         7\n",
            "weighted avg     0.6310    0.5714    0.5905         7\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEmCAYAAADfpHMGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de7xd853/8df75CSIxDUokaAto2pKNXUvQZti6qdm9FeKmpZJjd6U3sfQ6vTXmZreDKpRqtpKMaTVVpFeNCgqSSMkEVIUEUJCEpHgyOf3x/oeVo9z9l7nnLXPXvuc99NjPbL3Wmuv72fnyOd81/reFBGYmVn/tDU7ADOzwcDJ1MysBE6mZmYlcDI1MyuBk6mZWQmcTM3MSuBkat2StIGkX0haIenqflznOEk3lRlbs0h6h6SFzY7DqknuZ9raJH0AOB3YGVgFzAG+GhG39vO6JwAfB/aNiI5+B1pxkgLYMSIWNTsWa02umbYwSacD3wb+H7AVMB64EDiyhMtvB9w/FBJpEZLamx2DVVxEeGvBDdgYeA54X41z1iNLto+n7dvAeunYROAx4AxgKbAE+FA69mXgReClVMZJwJeAH+euvT0QQHt6/8/Ag2S144eA43L7b819bl/gLmBF+nPf3LGbga8At6Xr3ASM6eG7dcb/2Vz87wUOB+4HlgNfzJ2/J3A78Gw693xgRDo2I32X1en7vj93/c8BTwA/6tyXPvOGVMYe6f02wFPAxGb/v+GtOZtrpq1rH2B9YFqNc/4N2BvYHdiNLKGcmTv+OrKkPJYsYV4gadOIOJustntlRIyKiEtqBSJpQ+A84LCIGE2WMOd0c95mwK/SuZsD3wR+JWnz3GkfAD4EbAmMAD5do+jXkf0djAXOAi4GjgfeBrwD+HdJO6RzXwY+BYwh+7s7BDgVICIOSOfslr7vlbnrb0ZWS5+cLzgi/kKWaH8saSTwA+CHEXFzjXhtEHMybV2bA09H7dvw44BzImJpRDxFVuM8IXf8pXT8pYi4nqxW9nd9jGcdsKukDSJiSUTM6+acfwAeiIgfRURHREwF7gOOyJ3zg4i4PyLWAFeR/SLoyUtkz4dfAn5Klii/ExGrUvnzyX6JEBGzIuKOVO7DwPeAAwt8p7Mj4oUUz9+IiIuBRcCdwNZkv7xsiHIybV3LgDF1nuVtA/w19/6vad8r1+iSjJ8HRvU2kIhYTXZrfAqwRNKvJO1cIJ7OmMbm3j/Ri3iWRcTL6XVnsnsyd3xN5+cl7STpl5KekLSSrOY9psa1AZ6KiLV1zrkY2BX4n4h4oc65Nog5mbau24EXyJ4T9uRxslvUTuPTvr5YDYzMvX9d/mBE3BgR7yKrod1HlmTqxdMZ0+I+xtQb3yWLa8eI2Aj4IqA6n6nZ1UXSKLLn0JcAX0qPMWyIcjJtURGxguw54QWS3itppKThkg6T9PV02lTgTElbSBqTzv9xH4ucAxwgabykjYEvdB6QtJWkI9Oz0xfIHhes6+Ya1wM7SfqApHZJ7wd2AX7Zx5h6YzSwEngu1Zr/tcvxJ4HX9/Ka3wFmRsTJZM+CL+p3lNaynExbWER8g6yP6ZlkLcmPAh8DfpZO+Q9gJjAXuAeYnfb1pazpwJXpWrP42wTYluJ4nKyF+0Bem6yIiGXAe8h6ECwja4l/T0Q83ZeYeunTZI1bq8hqzVd2Of4l4IeSnpX0f+tdTNKRwKG8+j1PB/aQdFxpEVtLcad9M7MSuGZqZlYCJ1MzG3QkrS/pT5LuljRP0pe7OWc9SVdKWiTpTknb5459Ie1fKOndRcp0MjWzwegF4OCI2I2sr/Khkvbucs5JwDMR8UbgW8B/AUjaBTgGeDPZc/ELJQ2rV6CTqZkNOpF5Lr0dnrauDURHAj9Mr/8XOESS0v6fpsEaD5ENzNizXpmDavIGtW8QGjG62WFYL43ZZstmh2B9sGrpYtasfKZeX91eGbbRdhEdrxls1q1Y89Q8ID+oYkpETOl8k2qTs4A3AhdExJ1dLjGWrAcMEdEhaQXZyMKxwB258x7jbweWdGtwJdMRo1nv7+r2arGKed+/n9rsEKwPrv5s+f/WomNN4X/Da+dcsDYiJvR4rWx03O6SNgGmSdo1Iu4tKdTX8G2+mVWIQG3FtoIi4lng92TPP/MWA+PglSkWNybr//zK/mRbCozSczI1s+oQ0Das2FbrMtmov03S6w2Ad5ENJ867DjgxvT4a+F1kHe+vA45Jrf07ADsCf6oX+qC6zTezQUClPIbdmmxE2zCySuNVEfFLSeeQDQG+jmxOhR9JWkQ2cu8YgIiYJ+kqslnHOoCP5ibU6ZGTqZlViHp1C9+TiJgLvLWb/WflXq8F3tfD578KfLU3ZTqZmlm1lFMzHXBOpmZWHaKUmmkzOJmaWYXINVMzs1LUaamvKidTM6uQchqgmsHJ1MyqQ/g238ysFK6Zmpn1l2/zzczK0ebbfDOz/nE/UzOzMshdo8zMSuHWfDOzEvg238ysn+ThpGZm5XDN1MysBK6Zmpn1l1vzzcz6z/1MzczK4OGkZmblKOmZqaRxwOXAVkAAUyLiO13O+QxwXHrbDrwJ2CIilkt6GFgFvAx0RMSEWuU5mZpZtZRXM+0AzoiI2ZJGA7MkTY+I+Z0nRMS5wLkAko4APhURy3PXOCgini5SmJOpmVVLSTXTiFgCLEmvV0laAIwlW8K5O8cCU/taXms+nDCzwUmpNb/IBmMkzcxtk3u+rLYnW/r5zh6OjwQOBa7J7Q7gJkmzal27k2umZlYpKl4zfbrec8x0vVFkSfK0iFjZw2lHALd1ucXfPyIWS9oSmC7pvoiY0VM5rpmaWWVkq5ao0FboetJwskT6k4i4tsapx9DlFj8iFqc/lwLTgD1rleVkambVoV5s9S6VZdxLgAUR8c0a520MHAj8PLdvw9RohaQNgUnAvbXK822+mVVI8VpnAfsBJwD3SJqT9n0RGA8QERelfUcBN0XE6txntwKmpVjagSsi4oZahTmZmlmllJVMI+JWCtRhI+Iy4LIu+x4EdutNeU6mZlYpbW2t+fTRydTMqqPg89AqcjI1s8pQuc9MB5STqZlVipOpmVkJnEzNzErgZGpm1l8CtTmZmpn1ixugzMxK4mRqZlaG1sylTqZmViFyzdTMrBROpmZm/STksflmZqVozYqpk2nVrTeind9cchojRrTTPmwY037zZ/7jouubHZbVsckG7Zw4YSyj12sHglsfepab/7K87ueGPD8ztUZ54cUODp18HqvXvEh7exu/u/R0brptPn+65+Fmh2Y1rAu49p4nefTZtazX3sbnDtqB+5Y+xxOrXmx2aJXnZGoNs3pN9g9wePsw2tuHERFNjsjqWbm2g5VrOwB4oWMdT656kU02GO5kWoCTqTVMW5v44xWf4w3jtuB7V87grnv/2uyQrBc2GzmcbTdZn4eXr2l2KK2hNXPpwCyoJ2lbST+X9ICkv0j6jqQRA1H2YLBuXbD3Mf/JG999JhN23Y5d3rB1s0OygtYbJv5lr23537lPsLZjXbPDqTwpa80vshW41jhJv5c0X9I8SZ/s5pyJklZImpO2s3LHDpW0UNIiSZ+vV17Dk2laIfBa4GcRsSOwEzAK+Gqjyx5sVjy3hj/MvJ9J++7S7FCsgDbByXuP465HV3D346uaHU7LKHGp5w7gjIjYBdgb+Kik7v7x3BIRu6ftnBTDMOAC4DBgF+DYHj77ioGomR4MrI2IHwBExMvAp4APSzpV0rWSbki11q93fkjSJEm3S5ot6WpJowYg1soZs+koNh61AQDrrzecQ/bamYUPP9nkqKyI4/fYhidWvcDvFrkVvzfKSqYRsSQiZqfXq4AFwNiCYewJLIqIByPiReCnwJG1PjAQz0zfDMzK74iIlZIeSeXvDrwVeAFYKOl/gDXAmcA7I2K1pM8BpwPndL24pMnAZACGD758+7oxG3HxOScwrK2NtjZxzfTZ/PqWmst3WwW8YfMN2Gu7TVi8Yi1fOPj1AFw3bynznnyuyZG1gOLPTMdImpl7PyUipnR7SWl7sjxzZzeH95F0N/A48OmImEeWdB/NnfMYsFetYKrQAPXbiFgBIGk+sB2wCVnV+rb0G2gEcHt3H05/eVMA2kZuOeiaue994HH2Ofa/mh2G9dJflq3ho9fOb3YYLakXrflPR8SEAtcbBVwDnBYRK7scng1sFxHPSToc+BmwY2/i7TQQyXQ+cHR+h6SNgPFkzzReyB16OcUkYHpEHDsA8ZlZVZTcaV/ScLJE+pOIuLbr8XxyjYjrJV0oaQywGBiXO3XbtK9HA/HM9LfASEkfhFce7H4DuAx4vofP3AHsJ+mN6TMbStppAGI1syYSIBXb6l4ry8qXAAsi4ps9nPO6dB6S9iTLicuAu4AdJe2Qeh4dA1xXq7yGJ9PIepgfBbxP0gPA/cBa4Is1PvMU8M/AVElzyW7xd250rGbWbKKtrdhWwH7ACcDBua5Ph0s6RdIp6ZyjgXvTM9PzgGMi0wF8DLiRrOHqqvQstUcD8sw0Ih4Fjujm0GVp6zzvPbnXvwPe3ujYzKxayrrNj4hbqdOcFRHnA+f3cOx6oPBEGFVogDIzyxS8ha8iJ1MzqwxB0Vv4ynEyNbNKcc3UzKwEnjXKzKyfJN/mm5mVoPAkJpXjZGpmldKiudTJ1MyqxTVTM7P+cj9TM7P+y8bmt2Y2dTI1s0pxa76ZWQlatGLqZGpmFVLyfKYDycnUzCqjcz7TVuRkamYV4k77ZmalaNFc6mRqZhXisflmZv3nfqZmZiVp1WQ6EKuTmpkVVuLqpOMk/V7SfEnzJH2ym3OOkzRX0j2S/ihpt9yxh9P+OZJm1ivPNVMzq5QSa6YdwBkRMVvSaGCWpOkRMT93zkPAgRHxjKTDgCnAXrnjB0XE00UKczI1s+oocaKTiFgCLEmvV0laAIwF5ufO+WPuI3cA2/a1PN/mm1llCNHWVmwDxkiamdsm93hdaXvgrcCdNYo/Cfh17n0AN0maVevanVwzNbNKaSteNX06IibUO0nSKOAa4LSIWNnDOQeRJdP9c7v3j4jFkrYEpku6LyJm9Bh30ajNzAZCWQ1Q2bU0nCyR/iQiru3hnLcA3weOjIhlnfsjYnH6cykwDdizVllOpmZWGUoTnRTZ6l9LAi4BFkTEN3s4ZzxwLXBCRNyf279harRC0obAJODeWuX5Nt/MKqXEAVD7AScA90iak/Z9ERgPEBEXAWcBmwMXpgTdkR4dbAVMS/vagSsi4oZahTmZmlmllNU1KiJuJRtUVeuck4GTu9n/ILDbaz/Rsx6TqaT/IWvN6imIT/SmIDOzekSvGqAqpVbNtG6PfzOzsrXoPCc9J9OI+GH+vaSREfF840MysyGrYONSFdVtzZe0j6T5wH3p/W6SLmx4ZGY2JJXZNWogFeka9W3g3cAygIi4GzigkUGZ2dDU+cy0yFY1hVrzI+LRLlXvlxsTjpkNdRXMk4UUSaaPStoXiDSa4JPAgsaGZWZDkQb5TPunAN8hm23lceBG4KONDMrMhq4q3sIXUTeZprn8jhuAWMzMaveyr7Airfmvl/QLSU9JWirp55JePxDBmdnQU9bY/IFWpDX/CuAqYGtgG+BqYGojgzKzoSlrzS+2VU2RZDoyIn4UER1p+zGwfqMDM7MhqGCttIo101pj8zdLL38t6fPAT8nG6r8fuH4AYjOzIaiCebKQWg1Qs8iSZ+dX+0juWABfaFRQZjY0CRhWxXv4AmqNzd9hIAMxM4NSVycdUIVGQEnaFdiF3LPSiLi8UUGZ2dDVmqm0QDKVdDYwkSyZXg8cBtwKOJmaWamk1u20X6Q1/2jgEOCJiPgQ2ezTGzc0KjMbsgbzrFFrImId0CFpI2ApMK6xYZnZUFXignrjJP1e0nxJ8yR9sptzJOk8SYskzZW0R+7YiZIeSNuJ9cor8sx0pqRNgIvJWvifA24v8Dkzs14RKrM1vwM4IyJmp5VGZ0maHhHzc+ccBuyYtr2A7wJ7pa6hZwMTyHovzZJ0XUQ801NhRcbmn5peXiTpBmCjiJjbl29mZlZTibfwEbEEWJJer5K0gGzCpnwyPRK4PCICuEPSJpK2Jmsnmh4RywEkTQcOpcboz1qd9veodSwiZhf+VgPkrW8az213nt/sMMyGhDu+2piBkL3oGjVGUn6tuikRMaWHa24PvBW4s8uhscCjufePpX097e9RrZrpN2ocC+DgWhc2M+uLIg05ydNpjfuaJI0CrgFOi4iVfY+stlqd9g9qVKFmZt0R5XbaTxPaXwP8JCKu7eaUxfxtg/q2ad9islv9/P6ba5XVi18CZmaNV9asUcqy8iXAgoj4Zg+nXQd8MLXq7w2sSM9abwQmSdpU0qbApLSvR4VGQJmZDQSp1LH5+wEnAPdImpP2fREYDxARF5ENRDocWAQ8D3woHVsu6SvAXelz53Q2RvXEydTMKqWsXBoRt1JndGpqxe92GaaIuBS4tGh5RWbal6TjJZ2V3o+XtGfRAszMemMwj4C6ENgHODa9XwVc0LCIzGzIymbaV6Gtaorc5u8VEXtI+jNARDwjaUSD4zKzIapVW8WLJNOXJA0j61uKpC2AdQ2NysyGrApWOgspkkzPA6YBW0r6KtksUmc2NCozG5KkUsfmD6giY/N/ImkW2TR8At4bEQsaHpmZDUktmksLTQ49nqz/1S/y+yLikUYGZmZDT2cDVCsqcpv/K15dWG99YAdgIfDmBsZlZkNUi+bSQrf5f59/n2aTOrWH083M+q7gUNEq6vUIqDTR6l6NCMbMTC26pF6RZ6an5962AXsAjzcsIjMbsgS0t2hH0yI109G51x1kz1CvaUw4ZjbUlTkF30CqmUxTZ/3REfHpAYrHzIawrDW/2VH0Ta1lS9ojokPSfgMZkJkNYRWdxKSIWjXTP5E9H50j6TrgamB158EeZq02M+uXwdzPdH1gGdmaT539TQNwMjWzUg3K23yysfinA/fyahLtFA2NysyGKDFsENZMhwGj6H6maidTMytdtqBes6Pom1rJdElEnDNgkZiZDdIRUC36lcyslZXVACXpUuA9wNKI2LWb458Bjktv24E3AVukxfQeJltV5GWgIyIm1I27xrFDehm7mVm/dN7ml7QG1GXAoT0djIhzI2L3iNgd+ALwhy4rkB6UjtdNpFCjZlpvWVMzs0Yoq2YaETMkbV/w9GOBqf0pr0VHwZrZYCRgmIptwBhJM3Pb5D6VKY0kq8Hmh8kHcJOkWUWv2+tZo8zMGka9Gpv/dNFb8DqOAG7rcje+f0QslrQlMF3SfRExo9ZFXDM1s0pRwa1Ex9DlFj8iFqc/l5KtgbdnvYs4mZpZZXQuW1JkK6U8aWPgQODnuX0bShrd+RqYRDZ4qSbf5ptZpZRV65Q0FZhI9mz1MeBsYDhARFyUTjsKuCkiVuc+uhUwLT1uaAeuiIgb6pXnZGpmlVLWCKiIOLbAOZeRdaHK73sQ2K235TmZmlmFaHBODm1mNpCyrlFOpmZm/daaqdTJ1MyqpHf9TCvFydTMKkO0bn9NJ1MzqxTXTM3MStCaqdTJ1MwqxK35ZmYladFc6mRqZlUi1KI3+k6mZlYprpmamfVT1jWqNbOpk6mZVUfx9Z0qx8nUzCqlrLlKB1qrDjYYMj5y8ocZv82WvG3316xUaxXmn1vfZJNDF9uqxsm04k448Z/5+S/rzktrFeOfW9+p4H9V42Racfu/4wA222yzZodhveSfW99Jxbaq8TNTM6uUKtY6i2hYzVTSUZLmdNnWSTqsUWWaWWsr85mppEslLZXU7WJ4kiZKWpHLT2fljh0qaaGkRZI+XyT2htVMI2Ia2RKpncFNBo4Dbqz3WWXTxigi1jUqPjOroBJXHiVb2+l84PIa59wSEe/52xA0DLgAeBfwGHCXpOsiYn6twgbkmamknYCzgBMiYp2kz0i6S9JcSV9O52yffhNcTras6jhJ50q6V9I9kt4/ELGaWXOp4FZPRMwAlvchhD2BRRHxYES8CPwUOLLehxqeTCUNB64AzoiIRyRNAnYkC3h34G2SDkin7whcGBFvBiak47sB7wTOlbR1o+Otmg8efywT37EP9y9cyBu235bLLr2k2SFZAf659U12m69CW0n2kXS3pF9LenPaNxZ4NHfOY2lfTQPRAPUVYF5EXJneT0rbn9P7UWRJ9BHgrxFxR9q/PzA1Il4GnpT0B+DtwHX5i6fHB5MBxo0f38jv0RSX/3hqs0OwPvDPre96kSbHSJqZez8lIqb0oqjZwHYR8Zykw4GfkeWiPmloMpU0EfgnYI/8buBrEfG9LuduD6zubRnpL28KwNveNiH6GKqZVUXxbPp0REzoazERsTL3+npJF0oaAywGxuVO3Tbtq6mRrfmbAj8APhgRq3KHbgQ+LGlUOm+spC27ucQtwPslDZO0BXAA8KdGxWtm1TBQnfYlvS41diNpT7J8uAy4C9hR0g6SRgDH0OWOuDuNrJmeAmwJfLfLmi5fI3uGenva/xxwPPByl89PA/YB7gYC+GxEPNHAeM2sAsoaKippKjCR7HHAY8DZwHCAiLgIOBr4V0kdwBrgmIgIoEPSx8gqfsOASyNiXr3yGtk16mtkibMn3+lm3ysDmdOX+kzazGyoKCmZRsSxdY6fT9Z1qrtj1wPX96Y8j4Ays8rIuj215ggoJ1Mzq46KjrsvwsnUzCqlRXOpk6mZVUyLZlMnUzOrkFJHNw0oJ1Mzq4yi4+6ryMnUzKqlRbOpk6mZVYq7RpmZlaBFH5k6mZpZtbRoLnUyNbMKEahFq6ZOpmZWGcK3+WZmpWjRXOpkamYV06LZ1MnUzCrFXaPMzErgZ6ZmZiVo0VzqZGpm1ZG15rdmOnUyNbPqaOHJoRu2OqmZWV+o4Fb3OtKlkpZKureH48dJmivpHkl/lLRb7tjDaf8cSTOLxO1kambVUlY2hcuAQ2scfwg4MCL+HvgKMKXL8YMiYveImFCkMN/mm1mFqLSuURExQ9L2NY7/Mff2DmDb/pTnmqmZVYpUbAPGSJqZ2yb3o9iTgF/n3gdwk6RZRa/rmqmZVUYvx+Y/XfQWvGaZ0kFkyXT/3O79I2KxpC2B6ZLui4gZta7jmqmZVYoK/ldKWdJbgO8DR0bEss79EbE4/bkUmAbsWe9aTqZmVim9uM3vZzkaD1wLnBAR9+f2byhpdOdrYBLQbY+APN/mm1mllNXNVNJUYCLZs9XHgLOB4QARcRFwFrA5cGEaKNCRHhtsBUxL+9qBKyLihnrlOZmaWXWU2Gk/Io6tc/xk4ORu9j8I7PbaT9TmZGpmFdOaQ6CcTM2sMgS0tWYudTI1s2pp1bH5TqZmVimeHNrMrAytmUudTM2sWlo0lzqZmll1lNUhvxmcTM2sUjzTvplZCVozlTqZmlnFtGjF1MnUzKqkvBmhBpqTqZlVRi/nM60UT8FnZlYC10zNrFLaWrRq6mRqZtXhfqZmZv1XfBXn6nEyNbNqadFs6mRqZpXSql2j3JpvZpVS1oJ6ki6VtFRSt4vhKXOepEWS5kraI3fsREkPpO3EInE7mZpZpZS4OullwKE1jh8G7Ji2ycB3s/K1Gdnie3uRLfF8tqRN6xXmZGpmlaKC/9UTETOA5TVOORK4PDJ3AJtI2hp4NzA9IpZHxDPAdGonZcDPTM2sQgZ4BNRY4NHc+8fSvp721zSokuns2bOe3mC4/trsOBpkDPB0s4OwXhvMP7ftyr7g7NmzbtxguMYUPH19STNz76dExJSyYypqUCXTiNii2TE0iqSZETGh2XFY7/jn1jsRUfd2ukSLgXG599umfYuBiV3231zvYn5mamZD1XXAB1Or/t7AiohYAtwITJK0aWp4mpT21TSoaqZmZp0kTSWrYY6R9BhZC/1wgIi4CLgeOBxYBDwPfCgdWy7pK8Bd6VLnRESthqysvIgo+ztYA0ia3MznQdY3/rkNHU6mZmYl8DNTM7MSOJmamZXAydTMrAROpi1ErbqguPlnNwS4a1SLkKRIrYWSDgAeB9oi4v7mRmb1dPnZbQEQEU81Nyorm1vzW4yk04CjgD8AE4AzImJBc6OyIiR9BjiEbBjmd4EbI2Jhc6Oysvg2v4VI2g04PCIOBDYDVgMLJY1obmTWnfytvaSJwD8B7wVOBnYCjmhOZNYITqYV1s1ztjZggaRPA68HToiIdcDBkjYZ8ACtR11u7bclG3nzZESsjYjbgO8DH5G0XzPjtPI4mVZUl3+MG6fdc4E3A/8SEYdHxFpJ/wJ8ullxWvdyP7vjgJ+SzRy1XNJESetHxBzgV8DoJoZpJfIz0wrqkkhPJZuYdgHwdWBvsmem6wFzgOPJaqjdLs1gzSPpaODjwEkRsUjS2cCmwIvAw2S/BA+OiIebFqSVxjXTCsol0vcB/0iWRN8KfIps0tovA/eTTc7wASfSaujmsczLwP68Okv714DfAauANwH/4EQ6eLhmWiHp2doasudrI4AfAldGxEWSxgDfBJ4ALoiIwToJdkvqcjfxJmB5RDwp6QiyX4ZnRcTVufPbI6KjSeFaAziZVoSkI4HPA08CWwJLgfvIaqSfjYi709yKl5Dd8n8pIl5qVrz2KknDO38Wue5PK8l+fucB+wFnAd+IiCvSea8kXxscnEwrQNJBwPeAY4EHgc2By4E/AwvJVk+8OCLmplb7kRHxeLPitVdJOgQYFxGXSXon2S++SZKuAl4Cjo+ISM9PTyNbEfM5J9LBx8m0AiT9G9ks3+enlt61ksYDVwOPAL8H3g6cGxHzmxmrvUrSwWSztT8eETtJOpTsTqIDOBh4b0S8IGn3iJgjaVREPNfMmK1x3ADVRLkGi23JFl4DeEHSsIh4BDgJ2Jis5XchtZettQEk6d3AfwOn8uqM7A8B7yTrjP9/UiL9BPAVSSOdSAc3j81votyt3v8Cn5f0toiYJSkkDQeeTdtvyIYevtysWO1Vkt4FfBv4cETcLukzaXTaIuCPwEjgc5KWk412+kBEPN+8iG0guGZaDXcAtwHvlzQhItalBo19ga3InpE6kVaApHZgZ+DklEiHkTUWbhMRq4FvAb8EtiZrSDzGXdeGBj8zrQhJY8lqMQcDt5N17D4aOD5KsM8AAAOxSURBVDYi7m5mbPa3Ors1SWqLiHWSzgFWRsR/p+NHk9VOr4qItU0N1gaMa6YVERGLyfojnknWgLEcOMqJtHpy/UM7ayKrye4ikHQ8cA5wpxPp0OKaqVk/SXoL8GHgFrK+wie618XQ42Rq1k+pG9vDwANk3aE8v+wQ5Nt8s/5bAlxA1h3KiXSIcs3UrAT5IaU2NDmZmpmVwLf5ZmYlcDI1MyuBk6mZWQmcTM3MSuBkOgRJelnSHEn3Srpa0sh+XOuyNHwSSd+XtEuNcydK2rcPZTycVhootL/LOb2aqUnSl9Lqr2a94mQ6NK2JiN0jYleyOQBOyR9Mk3n0WkScXGfkz0TSsEuzwcbJ1G4B3phqjbdIug6YL2mYpHMl3SVprqSPQDYHq6TzJS2U9BuymZFIx26WNCG9PlTSbEl3S/qtpO3JkvanUq34HZK2kHRNKuOuzjXkJW0u6SZJ8yR9H+i6UN1rSPqZpFnpM5O7HPtW2v9bSVukfW+QdEP6zC2Sdi7jL9OGLs9nOoSlGuhhwA1p1x7ArhHxUEpIKyLi7ZLWA26TdBPZTPJ/B+xCNj3gfODSLtfdArgYOCBda7OIWC7pIrIlOzpnV7oC+FZE3JqGZN5Itmrn2cCtEXGOpH8gmyS7ng+nMjYA7pJ0TUQsAzYEZkbEpySdla79MWAKcEpEPCBpL+BCshm7zPrEyXRo2kDSnPT6FrJF+vYF/hQRD6X9k4C3dD4PJZvxf0fgAGBqml/1cUm/6+b6ewMzOq8VET2tEPBOYJdXFxxgI0mjUhn/mD77K0nPFPhOn5B0VHo9LsW6DFgHXJn2/xi4NpWxL3B1ruz1CpRh1iMn06FpTUTsnt+Rksrq/C7g4xFxY5fzDi8xjjZg765T1ek1y8/XJmkiWWLeJyKel3QzsH4Pp0cq99mufwdm/eFnptaTG4F/TcunIGknSRsCM8hWBBgmaWvgoG4+ewdwgKQd0mc3S/tXAaNz590EfLzzjaTO5DYD+EDadxiwaZ1YNwaeSYl0Z7Kacac2skm2Sde8NSJWAg9Jel8qQ2nZEbM+czK1nnyf7HnobEn3ki1F3Q5MI5tqbj7ZctS3d/1gRDwFTCa7pb6bV2+zfwEc1dkABXwCmJAauObzaq+CL5Ml43lkt/uP1In1BqBd0gLgP8mSeafVwJ7pOxxMNnEzwHHASSm+ecCRBf5OzHrkiU7MzErgmqmZWQmcTM3MSuBkamZWAidTM7MSOJmamZXAydTMrAROpmZmJfj/w0qVhokib/cAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkEwFuLG0VmI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LGnyQyI0xva"
      },
      "source": [
        "## Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVu6ik_J09OJ"
      },
      "source": [
        "### Batch Perceptron Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXjQV3050_Z1"
      },
      "source": [
        "class BatchPerceptronLearning:\n",
        "    def fit(X, Y, a, n):\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Applying Sample Normalisation:\n",
        "        Norm_Y = []\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "            if y != 1:\n",
        "                x = [i * -1 for i in x]\n",
        "                x.insert(0, -1)\n",
        "                Norm_Y.append(x)\n",
        "            else:\n",
        "                x.insert(0, 1)\n",
        "                Norm_Y.append(x)\n",
        "\n",
        "        print(\"Vectors used in Batch Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Batch Perceptron Learning Algorithm:\n",
        "\n",
        "        epoch = 1\n",
        "\n",
        "        while True:\n",
        "\n",
        "            updating_samples = []\n",
        "            print(\"Epoch {}\".format(epoch))\n",
        "\n",
        "            for count, i in enumerate(range(len(Norm_Y))):\n",
        "\n",
        "                # Knowing which value of a to use. If it is the first iteration, than use the given parameters in the \n",
        "                # question:\n",
        "                a_prev = a\n",
        "                print(\"The value of a used is {}\".format(a_prev))\n",
        "                y_input = Norm_Y[i]\n",
        "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
        "\n",
        "                # Equation -> g(x) = a^{t}y\n",
        "                ay = np.dot(a, y_input)\n",
        "                print(\"The value of g(x) i.e. a^t*y for this iteration is: {}\".format(ay))\n",
        "\n",
        "\n",
        "                # Checking if the sample is misclassified or not:\n",
        "\n",
        "                # If sample is misclassified:\n",
        "                if ay <= 0:\n",
        "\n",
        "                    # If this is the first sample in the epoch, add the previous value of a to the list of samples used \n",
        "                    # for the update to perform summation at the end of the epoch:\n",
        "                    if count == 0:\n",
        "                        print(\"YES, This sample is misclassified. This sample will be used in update.\\n\")\n",
        "                        updating_samples.append(np.array(a))\n",
        "                        updating_samples.append(np.array(y_input))\n",
        "\n",
        "                    # If sample is misclassified and IS NOT the first sample in the epoch:\n",
        "                    else:\n",
        "                        print(\"YES, This sample is misclassified. This sample will be used in update.\\n\")\n",
        "                        updating_samples.append(np.array(y_input))\n",
        "\n",
        "                # If sample is classified correctly:\n",
        "                else: \n",
        "\n",
        "                    # If first sample in the epoch, append the previous value of a to the updating samples list:\n",
        "                    if count == 0:\n",
        "                        updating_samples.append(np.array(a))\n",
        "                        print(\"NO, This sample is not mis-classified.\\n\")\n",
        "                    else:\n",
        "                        print(\"NO, This sample is not mis-classified.\")\n",
        "\n",
        "            # Calculating new value of a after having gone through all of the samples in the dataset since it is Batch Learning.\n",
        "            a_update_val = n * sum(updating_samples)\n",
        "\n",
        "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
        "            # to update the parameters, we can conclude that learning has converged.\n",
        "            if len(updating_samples) <= 1:\n",
        "                print(\"\\nLearning has converged.\")\n",
        "                print(\"Required parameters of a are: {}\".format(a))\n",
        "                break\n",
        "\n",
        "            # Updating a using our new value of a:\n",
        "            a = a_update_val\n",
        "            print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
        "\n",
        "            epoch += 1\n",
        "        \n",
        "    #def setParams():\n",
        "        #X_train = transformToList(input(\"Enter features eg [[1, 5], [2, 5], [4, 1], [5, 1]] 2D:  \"))\n",
        "        #y_train = transformToList(input(\"Enter labels eg [1, 1, 2, 2] 1D:  \"))\n",
        "        #a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [-25, 6, 3] 1D:  \"))\n",
        "        #lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
        "        #return X_train, y_train, a, lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79ADojng1X_o",
        "outputId": "8bdb1abd-cfd6-4e79-d1d5-9cc524b43ae4"
      },
      "source": [
        "X_train = [[1,5],[2,5],[4,1],[5,1]]\n",
        "y_train = [1,1,2,2,2]\n",
        "a = [-25,6,3]\n",
        "lr = 1\n",
        "BatchPerceptronLearning.fit(X_train, y_train, a, lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors used in Batch Perceptron Learning Algorithm:\n",
            " [[1, 1, 5], [1, 2, 5], [-1, -4, -1], [-1, -5, -1]]\n",
            "\n",
            "Epoch 1\n",
            "The value of a used is [-25, 6, 3]\n",
            "y Value used for this iteration is: [1, 1, 5]\n",
            "The value of g(x) a^t*y for this iteration is: -4\n",
            "YES, This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "The value of a used is [-25, 6, 3]\n",
            "y Value used for this iteration is: [1, 2, 5]\n",
            "The value of g(x) a^t*y for this iteration is: 2\n",
            "NO, This sample is not mis-classified.\n",
            "The value of a used is [-25, 6, 3]\n",
            "y Value used for this iteration is: [-1, -4, -1]\n",
            "The value of g(x) a^t*y for this iteration is: -2\n",
            "YES, This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "The value of a used is [-25, 6, 3]\n",
            "y Value used for this iteration is: [-1, -5, -1]\n",
            "The value of g(x) a^t*y for this iteration is: -8\n",
            "YES, This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "\n",
            "New Value of a^t is: [-26  -2   6].\n",
            "\n",
            "Epoch 2\n",
            "The value of a used is [-26  -2   6]\n",
            "y Value used for this iteration is: [1, 1, 5]\n",
            "The value of g(x) a^t*y for this iteration is: 2\n",
            "NO, This sample is not mis-classified.\n",
            "\n",
            "The value of a used is [-26  -2   6]\n",
            "y Value used for this iteration is: [1, 2, 5]\n",
            "The value of g(x) a^t*y for this iteration is: 0\n",
            "YES, This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "The value of a used is [-26  -2   6]\n",
            "y Value used for this iteration is: [-1, -4, -1]\n",
            "The value of g(x) a^t*y for this iteration is: 28\n",
            "NO, This sample is not mis-classified.\n",
            "The value of a used is [-26  -2   6]\n",
            "y Value used for this iteration is: [-1, -5, -1]\n",
            "The value of g(x) a^t*y for this iteration is: 30\n",
            "NO, This sample is not mis-classified.\n",
            "\n",
            "New Value of a^t is: [-25   0  11].\n",
            "\n",
            "Epoch 3\n",
            "The value of a used is [-25   0  11]\n",
            "y Value used for this iteration is: [1, 1, 5]\n",
            "The value of g(x) a^t*y for this iteration is: 30\n",
            "NO, This sample is not mis-classified.\n",
            "\n",
            "The value of a used is [-25   0  11]\n",
            "y Value used for this iteration is: [1, 2, 5]\n",
            "The value of g(x) a^t*y for this iteration is: 30\n",
            "NO, This sample is not mis-classified.\n",
            "The value of a used is [-25   0  11]\n",
            "y Value used for this iteration is: [-1, -4, -1]\n",
            "The value of g(x) a^t*y for this iteration is: 14\n",
            "NO, This sample is not mis-classified.\n",
            "The value of a used is [-25   0  11]\n",
            "y Value used for this iteration is: [-1, -5, -1]\n",
            "The value of g(x) a^t*y for this iteration is: 14\n",
            "NO, This sample is not mis-classified.\n",
            "\n",
            "Learning has converged.\n",
            "Required parameters of a are: [-25   0  11]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ac2N6hHlrS4"
      },
      "source": [
        "### Sequential Perceptron Learning with Sample Normalization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5s0Ko3g610pS"
      },
      "source": [
        "class SequentialPerceptronLearningWithSampleNorm:\n",
        "    def fit(X, Y, a, n):\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Applying Sample Normalisation:\n",
        "        Norm_Y = []\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "            if y != 1:\n",
        "                x = [i * -1 for i in x]\n",
        "                x.insert(0, -1)\n",
        "                Norm_Y.append(x)\n",
        "            else:\n",
        "                x.insert(0, 1)\n",
        "                Norm_Y.append(x)\n",
        "\n",
        "        print(\"Vectors used in Sequential Perceptron Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Sequential Perceptron Learning Algorithm:\n",
        "\n",
        "        epoch = 1\n",
        "\n",
        "        while True:\n",
        "\n",
        "            updating_samples = []\n",
        "            print(\"Epoch {}\".format(epoch))\n",
        "\n",
        "            # Keeping track of how many samples are correctly classified. If this variable reaches \n",
        "            # the value that is equal to the size of the dataset (len), than we know that learning \n",
        "            # has converged:\n",
        "            correctly_classified_counter = 0\n",
        "\n",
        "            # Going through all of the samples in the dataset one-by-one:\n",
        "            for i in range(len(Norm_Y)):\n",
        "\n",
        "                # This chooses which weight to use for an iteration. If first iteration, uses given starting weight \n",
        "                # as described in question:\n",
        "                a_prev = a\n",
        "                print(\"The value of a used is {}\".format(a_prev))\n",
        "\n",
        "                # Selecting sample to use:\n",
        "                y_input = Norm_Y[i]\n",
        "                print(\"y Value used for this iteration is: {}\".format(y_input))\n",
        "\n",
        "                # Equation -> g(x) = a^{t}y\n",
        "                ay = np.dot(a, y_input)\n",
        "                print(\"The value of g(x) i.e. a^t*y for this iteration is: {}\".format(ay))\n",
        "\n",
        "\n",
        "                # Checking if the sample is misclassified or not:\n",
        "\n",
        "                # If sample is misclassified:\n",
        "                if ay <= 0:\n",
        "\n",
        "                    print(\"This sample is misclassified. This sample will be used in update.\\n\")\n",
        "                    updating_samples.append(np.array(a))\n",
        "                    updating_samples.append(np.array(y_input))\n",
        "\n",
        "                    # Calculating new value of a using update rule for Sequential Perceptron Learning Algorithm:\n",
        "                    a_update_val = n * sum(updating_samples)\n",
        "\n",
        "                    a = a_update_val\n",
        "                    print(\"\\nNew Value of a^t is: {}.\\n\".format(a))\n",
        "\n",
        "                # If the sample is correctly classified:\n",
        "                else: \n",
        "                    print(\"This sample is classified correctly.\\n\")\n",
        "                    correctly_classified_counter += 1\n",
        "                    pass\n",
        "\n",
        "                # Reset sample to add for update to occur:\n",
        "                updating_samples = []\n",
        "\n",
        "            # If Block to check whether learning has converged. If we have gone through all the data without needing \n",
        "            # to update the parameters, we can conclude that learning has converged.\n",
        "            if correctly_classified_counter == len(Norm_Y):\n",
        "                print(\"\\nLearning has converged.\")\n",
        "                print(\"Required parameters of a are: {}.\".format(a))\n",
        "                break\n",
        "\n",
        "            epoch += 1\n",
        "        \n",
        "    # def setParams():\n",
        "    #     X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
        "    #     y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
        "    #     a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
        "    #     lr = float(input(\"Enter the learning rate eg 1.  \"))\n",
        "    #     return X_train, y_train, a, lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ywne4JqjlgbU",
        "outputId": "ad55aa02-f291-4812-fcd3-57f3cdd66e36"
      },
      "source": [
        "X_train = [[1,5],[2,5],[4,1],[5,1]]\n",
        "y_train = [1,1,2,2,2]\n",
        "a = [-25,6,3]\n",
        "lr = 1\n",
        "SequentialPerceptronLearningWithSampleNorm.fit(X_train, y_train, a, lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors used in Sequential Perceptron Learning Algorithm:\n",
            " [[1, 1, 5], [1, 2, 5], [-1, -4, -1], [-1, -5, -1]]\n",
            "\n",
            "Epoch 1\n",
            "The value of a used is [-25, 6, 3]\n",
            "y Value used for this iteration is: [1, 1, 5]\n",
            "The value of a^t*y for this iteration is: -4\n",
            "This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "\n",
            "New Value of a^t is: [-24   7   8].\n",
            "\n",
            "The value of a used is [-24   7   8]\n",
            "y Value used for this iteration is: [1, 2, 5]\n",
            "The value of a^t*y for this iteration is: 30\n",
            "This sample is classified correctly.\n",
            "\n",
            "The value of a used is [-24   7   8]\n",
            "y Value used for this iteration is: [-1, -4, -1]\n",
            "The value of a^t*y for this iteration is: -12\n",
            "This sample is misclassified. This sample will be used in update.\n",
            "\n",
            "\n",
            "New Value of a^t is: [-25   3   7].\n",
            "\n",
            "The value of a used is [-25   3   7]\n",
            "y Value used for this iteration is: [-1, -5, -1]\n",
            "The value of a^t*y for this iteration is: 3\n",
            "This sample is classified correctly.\n",
            "\n",
            "Epoch 2\n",
            "The value of a used is [-25   3   7]\n",
            "y Value used for this iteration is: [1, 1, 5]\n",
            "The value of a^t*y for this iteration is: 13\n",
            "This sample is classified correctly.\n",
            "\n",
            "The value of a used is [-25   3   7]\n",
            "y Value used for this iteration is: [1, 2, 5]\n",
            "The value of a^t*y for this iteration is: 16\n",
            "This sample is classified correctly.\n",
            "\n",
            "The value of a used is [-25   3   7]\n",
            "y Value used for this iteration is: [-1, -4, -1]\n",
            "The value of a^t*y for this iteration is: 6\n",
            "This sample is classified correctly.\n",
            "\n",
            "The value of a used is [-25   3   7]\n",
            "y Value used for this iteration is: [-1, -5, -1]\n",
            "The value of a^t*y for this iteration is: 3\n",
            "This sample is classified correctly.\n",
            "\n",
            "\n",
            "Learning has converged.\n",
            "Required parameters of a are: [-25   3   7].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBnzTiAZtYc7"
      },
      "source": [
        "### Sequential Perceptron without sample normalisation\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pCAkrh9qpj8"
      },
      "source": [
        "# Sequential Perceptron Learning Algorithm\n",
        "\n",
        "# INPUTS:\n",
        "    # x: data\n",
        "    # y: target\n",
        "    # a: initial weights\n",
        "    # eta: learning rate\n",
        "    # n_epochs: number of epochs of learning on the entire dataset\n",
        "    # augmentation: True if data is augmented\n",
        "    # sample_normalisation: True if sample normalisation applied\n",
        "\n",
        "def sequential_perceptron_learning_without_norm(x, y, a, eta, n_epochs, augmentation, sample_normalisation):\n",
        "    n_instances = len(x)\n",
        "    \n",
        "    if augmentation:\n",
        "        x = np.insert(x, 0, 1, axis=1)\n",
        "    \n",
        "    if sample_normalisation: # CAREFUL WITH THE DATA LABEL\n",
        "        x[y==-1] = -x[y==-1]\n",
        "    \n",
        "    if sample_normalisation:\n",
        "        for n in range(n_epochs):\n",
        "            print(\"Epoch #{}: \".format(n+1))\n",
        "            for i in range(n_instances):\n",
        "                g_x = np.dot(a,x[i])\n",
        "                print('The value of g(x) i.e. a^t*y for this iteration is:', g_x) \n",
        "                if np.sign(np.dot(a, x[i])) <= 0: # only misclassified samples\n",
        "                    a += eta*x[i]\n",
        "                print(\"Sample #{}: {}\".format(i+1, a))\n",
        "            print(\"End of epoch #{}\\n\".format(n+1))\n",
        "\n",
        "\n",
        "    else:\n",
        "        for n in range(n_epochs):\n",
        "            print(\"Epoch #{}: \".format(n+1))\n",
        "            for i in range(n_instances):\n",
        "              g_x = np.dot(a,x[i])\n",
        "              print('The value of g(x) i.e. a^t*y for this iteration is:', g_x)\n",
        "              if np.sign(np.dot(a, x[i])) != y[i]: # only misclassified samples\n",
        "                  a += eta*y[i]*x[i]\n",
        "              print(\"Sample #{}: {}\".format(i+1, a))\n",
        "            print(\"End of epoch #{}\\n\".format(n+1))\n",
        "\n",
        "    return a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZzLgu5S5rWl",
        "outputId": "6a5235c5-bab7-41ce-f1bf-990eaf466e23"
      },
      "source": [
        "# Tutorial example (exercise 9)\n",
        "x = np.array([[0,0],[1,-1],[0.5,0.5],[1,0.5]]) # data\n",
        "y = np.array( [1,1,-1,-1]) # targets\n",
        "a = np.array([0.4,1.0,2.0]) # initial weights\n",
        "\n",
        "# And here we estimate the new vector\n",
        "a = sequential_perceptron_learning_without_norm(x, y, a, 1, 5, True, False)\n",
        "print(a) # It should be [1   3   1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1: \n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.4\n",
            "Sample #1: [0.4 1.  2. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #2: [1.4 2.  1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 2.9\n",
            "Sample #3: [0.4 1.5 0.5]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 2.15\n",
            "Sample #4: [-0.6  0.5  0. ]\n",
            "End of epoch #1\n",
            "\n",
            "Epoch #2: \n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #1: [0.4 0.5 0. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.8999999999999999\n",
            "Sample #2: [0.4 0.5 0. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.6499999999999999\n",
            "Sample #3: [-0.6  0.  -0.5]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.8500000000000001\n",
            "Sample #4: [-0.6  0.  -0.5]\n",
            "End of epoch #2\n",
            "\n",
            "Epoch #3: \n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #1: [ 0.4  0.  -0.5]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.8999999999999999\n",
            "Sample #2: [ 0.4  0.  -0.5]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.1499999999999999\n",
            "Sample #3: [-0.6 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -1.6\n",
            "Sample #4: [-0.6 -0.5 -1. ]\n",
            "End of epoch #3\n",
            "\n",
            "Epoch #4: \n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #1: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.8999999999999999\n",
            "Sample #2: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.3500000000000001\n",
            "Sample #3: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #4: [ 0.4 -0.5 -1. ]\n",
            "End of epoch #4\n",
            "\n",
            "Epoch #5: \n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.3999999999999999\n",
            "Sample #1: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: 0.8999999999999999\n",
            "Sample #2: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.3500000000000001\n",
            "Sample #3: [ 0.4 -0.5 -1. ]\n",
            "The value of g(x) i.e. a^t*y for this iteration is: -0.6000000000000001\n",
            "Sample #4: [ 0.4 -0.5 -1. ]\n",
            "End of epoch #5\n",
            "\n",
            "[ 0.4 -0.5 -1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Gz29uyW-m8C"
      },
      "source": [
        "def sequential_multiclass_perceptron_learning (N, augmented_matrix, eta, omega, number_of_classes, number_of_features ):\n",
        "  # N is the number of exemplars provided in the question\n",
        "  # augmented_matrix is the augmented feature vector from the question\n",
        "  # eta is the learning rate given in the question\n",
        "  # omega is an array containing all the output classes of the feature vectors\n",
        "\n",
        "  N_counter = 0 # counter which keeps track of cases where winner_class == omega[index]\n",
        "\n",
        "  #Step 2. Initialise aj for each class\n",
        "  at = np.zeros((number_of_classes, number_of_features))\n",
        "\n",
        "  for i in range(0,15):\n",
        "    print ('Iteration: ',i+1)\n",
        "    # Step 3. Find values of g1, g2 and g3 and then select the arg max of g\n",
        "    index = i % 5\n",
        "\n",
        "    #Print updated a^t value\n",
        "    print('a^t:')\n",
        "    print(at)\n",
        "    \n",
        "    # Compute g value\n",
        "    g = np.empty([number_of_classes])\n",
        "    for i in range(len(g)):\n",
        "      print('Calculation of g values..........')\n",
        "      print('a^t is:',at[i])\n",
        "      print('Index is:', index)\n",
        "      print('Aug matrix is:', augmented_matrix[:,index] )\n",
        "      g[i] = at[i] @ augmented_matrix[:,index]\n",
        "\n",
        "\n",
        "    print('g1 | g2 | g3')\n",
        "    print(g)\n",
        "\n",
        "    #Step 4. Select the winner\n",
        "    #Logic for 0,0,0 case and similar ones where 2 gs can produce max value\n",
        "    seen = []\n",
        "    bRepeated = False\n",
        "    # Check if there are multiple max values, and assign the winner class accordingly\n",
        "    for number in g:\n",
        "        if number in seen:\n",
        "          bRepeated = True\n",
        "          print (\"Number repeated!\")\n",
        "          m = max(g)\n",
        "          temp = [index for index, j in enumerate(g) if j == m]\n",
        "          winner_class = max(temp) + 1\n",
        "        else:\n",
        "            seen.append(number)\n",
        "    #If all g values are unique, simply select the max value's class as the winner\n",
        "    if(bRepeated == False):\n",
        "      g = g.tolist()\n",
        "      arg_max = max(g)\n",
        "      winner_class = g.index(arg_max) + 1\n",
        "    \n",
        "    print('Winner class = ', winner_class, ', and actual class is:',omega[index])\n",
        "\n",
        "    #Compare winnner to actual class \n",
        "    if(winner_class != omega[index]):\n",
        "      # Step 4. Apply the update rule as per the algorithm \n",
        "      \n",
        "      #Increment the actual class value which is incorrectly classified \n",
        "      at[omega[index]-1] = at[omega[index]-1] + eta * augmented_matrix[:,index]\n",
        "      print('New loser value:', at[omega[index]-1])\n",
        "\n",
        "      #Penalize the wrongly predicted Winner class\n",
        "      at[winner_class-1] = at[winner_class-1] - eta * augmented_matrix[:,index]\n",
        "      print('New winner value:', at[winner_class-1])\n",
        "\n",
        "      #Reset counter to 0\n",
        "      N_counter =0\n",
        "    else:\n",
        "      print ('No update is performed!')\n",
        "      N_counter +=1 #Increment convergence counter which keeps track of cases where winner_class == omega[index]\n",
        "      if(N_counter == N): ## check for convergence\n",
        "        print('Value of N = ', N)\n",
        "        print('Value of N_counter = ', N_counter)\n",
        "        print('Learning has converged, so stopping...')\n",
        "        print ('Final values of a^t after update....')\n",
        "        print('at')\n",
        "        print(at)\n",
        "        break\n",
        "      print ('N counter value = ', N_counter)\n",
        "    print('at')\n",
        "    print(at)\n",
        "    print ('=========================================================')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW3Qmnx5-u_I",
        "outputId": "abb49d80-d384-436f-8bff-ad1c80c2e3e8"
      },
      "source": [
        "N = 5 # N refers to the number of exemplars in the input dataset\n",
        "eta = 1\n",
        "input_array = np.array([[ 1,  1,  1,  1],\n",
        "                        [ 2,  0, -1, -1],\n",
        "                        [ 0,  2,  1, -1]]) # Input matrix from the question\n",
        "augmented_matrix = np.insert(input_array,0,1,axis=1)\n",
        "omega = np.array([1,1,2,2,3]) # Class labels from the question \n",
        "number_of_classes = 3\n",
        "number_of_features = 3\n",
        "\n",
        "sequential_multiclass_perceptron_learning (N, augmented_matrix, eta, omega, number_of_classes, number_of_features)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration:  1\n",
            "a^t:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [0. 0. 0.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [0. 0. 0.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [0. 0. 0.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "g1 | g2 | g3\n",
            "[0. 0. 0.]\n",
            "Number repeated!\n",
            "Number repeated!\n",
            "Winner class =  3 , and actual class is: 1\n",
            "New loser value: [1. 1. 1.]\n",
            "New winner value: [-1. -1. -1.]\n",
            "at\n",
            "[[ 1.  1.  1.]\n",
            " [ 0.  0.  0.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  2\n",
            "a^t:\n",
            "[[ 1.  1.  1.]\n",
            " [ 0.  0.  0.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 1. 1.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "Calculation of g values..........\n",
            "a^t is: [0. 0. 0.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "g1 | g2 | g3\n",
            "[ 3.  0. -3.]\n",
            "Winner class =  1 , and actual class is: 1\n",
            "No update is performed!\n",
            "N counter value =  1\n",
            "at\n",
            "[[ 1.  1.  1.]\n",
            " [ 0.  0.  0.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  3\n",
            "a^t:\n",
            "[[ 1.  1.  1.]\n",
            " [ 0.  0.  0.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 1. 1.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "Calculation of g values..........\n",
            "a^t is: [0. 0. 0.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "g1 | g2 | g3\n",
            "[ 3.  0. -3.]\n",
            "Winner class =  1 , and actual class is: 2\n",
            "New loser value: [1. 0. 2.]\n",
            "New winner value: [ 0.  1. -1.]\n",
            "at\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  4\n",
            "a^t:\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0.  1. -1.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 0. 2.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "g1 | g2 | g3\n",
            "[-2.  3. -1.]\n",
            "Winner class =  2 , and actual class is: 2\n",
            "No update is performed!\n",
            "N counter value =  1\n",
            "at\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  5\n",
            "a^t:\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0.  1. -1.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 0. 2.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "g1 | g2 | g3\n",
            "[ 0. -1.  1.]\n",
            "Winner class =  3 , and actual class is: 3\n",
            "No update is performed!\n",
            "N counter value =  2\n",
            "at\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  6\n",
            "a^t:\n",
            "[[ 0.  1. -1.]\n",
            " [ 1.  0.  2.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0.  1. -1.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 0. 2.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "g1 | g2 | g3\n",
            "[ 0.  3. -3.]\n",
            "Winner class =  2 , and actual class is: 1\n",
            "New loser value: [1. 2. 0.]\n",
            "New winner value: [ 0. -1.  1.]\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  7\n",
            "a^t:\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 2. 0.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0. -1.  1.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 1\n",
            "Aug matrix is: [1 2 0]\n",
            "g1 | g2 | g3\n",
            "[ 5. -2. -3.]\n",
            "Winner class =  1 , and actual class is: 1\n",
            "No update is performed!\n",
            "N counter value =  1\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  8\n",
            "a^t:\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 2. 0.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0. -1.  1.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 2\n",
            "Aug matrix is: [1 0 2]\n",
            "g1 | g2 | g3\n",
            "[ 1.  2. -3.]\n",
            "Winner class =  2 , and actual class is: 2\n",
            "No update is performed!\n",
            "N counter value =  2\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  9\n",
            "a^t:\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 2. 0.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0. -1.  1.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 3\n",
            "Aug matrix is: [ 1 -1  1]\n",
            "g1 | g2 | g3\n",
            "[-1.  2. -1.]\n",
            "Number repeated!\n",
            "Winner class =  2 , and actual class is: 2\n",
            "No update is performed!\n",
            "N counter value =  3\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  10\n",
            "a^t:\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 2. 0.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0. -1.  1.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 4\n",
            "Aug matrix is: [ 1 -1 -1]\n",
            "g1 | g2 | g3\n",
            "[-1.  0.  1.]\n",
            "Winner class =  3 , and actual class is: 3\n",
            "No update is performed!\n",
            "N counter value =  4\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "=========================================================\n",
            "Iteration:  11\n",
            "a^t:\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n",
            "Calculation of g values..........\n",
            "a^t is: [1. 2. 0.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [ 0. -1.  1.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "Calculation of g values..........\n",
            "a^t is: [-1. -1. -1.]\n",
            "Index is: 0\n",
            "Aug matrix is: [1 1 1]\n",
            "g1 | g2 | g3\n",
            "[ 3.  0. -3.]\n",
            "Winner class =  1 , and actual class is: 1\n",
            "No update is performed!\n",
            "Value of N =  5\n",
            "Value of N_counter =  5\n",
            "Learning has converged, so stopping...\n",
            "Final values of a^t after update....\n",
            "at\n",
            "[[ 1.  2.  0.]\n",
            " [ 0. -1.  1.]\n",
            " [-1. -1. -1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLQlnkGtBZMU"
      },
      "source": [
        "### Moore-Penrose Pseudoinverse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfE24hGmBbEI"
      },
      "source": [
        "class Pseudoinverse:\n",
        "    def fit(X, Y, b):\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Applying Sample Normalisation:\n",
        "        Norm_Y = []\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "            if y == -1 or y == 2:\n",
        "                x = [i * -1 for i in x]\n",
        "                x.insert(0, y)\n",
        "                Norm_Y.append(x)\n",
        "            else:\n",
        "                x.insert(0, y)\n",
        "                Norm_Y.append(x)\n",
        "\n",
        "        print(\"Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Initialising Y Matrix:\n",
        "        Y_matrix = []\n",
        "\n",
        "        # Adding each normalised sample in dataset to Y Matrix:\n",
        "        for i in range(len(Norm_Y)):\n",
        "            Y_matrix.append(Norm_Y[i])\n",
        "        Y_matrix = np.array(Y_matrix)\n",
        "        print(\"y Matrix being used:\\n {}\\n\".format(Y_matrix))\n",
        "\n",
        "        # Calculating pseudo-inverse of Y Matrix:\n",
        "        pseudo_inv_matrix = np.linalg.pinv(Y_matrix)\n",
        "        print(\"Pseudo-inverse Matrix is:\\n {}\\n\".format(pseudo_inv_matrix))\n",
        "\n",
        "        # Multiplying Pseudo-inverse matrix by given margin vector in question:\n",
        "        a = np.matmul(pseudo_inv_matrix, b)\n",
        "        print(\"a is equal to:\\n {}\\n\".format(a))\n",
        "\n",
        "        correct_classification = 0\n",
        "\n",
        "        # Checking if classifications are correct:\n",
        "\n",
        "        for sample in Norm_Y:\n",
        "            ay = np.dot(sample, a)\n",
        "            print(\"\\ng(x) for sample {} is {}\".format(sample, ay))\n",
        "\n",
        "            # Sample is correctly classified if ay is positive:    \n",
        "            if ay > 0:\n",
        "                print(\"Sample has been correctly classified.\")\n",
        "                correct_classification += 1\n",
        "\n",
        "        if correct_classification == len(Norm_Y):\n",
        "            print(\"\\nAll samples are classified correctly which means that discriminant function parameters are correct.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nSome samples are misclassified.\")\n",
        "        \n",
        "    # def setParams():\n",
        "    #     X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
        "    #     y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
        "    #     b = transformToList(input(\"Enter b eg [1, 1, 1, 1, 1, 1] 1D:  \"))\n",
        "    #     return X_train, y_train, b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xixk1kFFBiRz",
        "outputId": "bac2611f-62f4-4fc0-fbed-69c52f90e87c"
      },
      "source": [
        "X_train =  [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]\n",
        "y_train =  [1,1,1,-1,-1,-1]\n",
        "b = [1,1,1,1,1,1]\n",
        "Pseudoinverse.fit(X_train, y_train, b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors used in Pseudoinverse operation to calculate parameters of linear discriminant function:\n",
            " [[1, 0, 2], [1, 1, 2], [1, 2, 1], [-1, 3, -1], [-1, 2, 1], [-1, 3, 2]]\n",
            "\n",
            "y Matrix being used:\n",
            " [[ 1  0  2]\n",
            " [ 1  1  2]\n",
            " [ 1  2  1]\n",
            " [-1  3 -1]\n",
            " [-1  2  1]\n",
            " [-1  3  2]]\n",
            "\n",
            "Pseudo-inverse Matrix is:\n",
            " [[ 0.0682  0.1648  0.3807  0.1023 -0.233  -0.2557]\n",
            " [-0.0341  0.0426  0.1847  0.1989 -0.0085  0.0028]\n",
            " [ 0.1402  0.0748 -0.1203 -0.2064  0.1184  0.1828]]\n",
            "\n",
            "a is equal to:\n",
            " [0.2273 0.3864 0.1894]\n",
            "\n",
            "\n",
            "g(x) for sample [1, 0, 2] is 0.6060606060606065\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [1, 1, 2] is 0.9924242424242433\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [1, 2, 1] is 1.1893939393939406\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 3, -1] is 0.7424242424242435\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 2, 1] is 0.7348484848484861\n",
            "Sample has been correctly classified.\n",
            "\n",
            "g(x) for sample [-1, 3, 2] is 1.3106060606060626\n",
            "Sample has been correctly classified.\n",
            "\n",
            "All samples are classified correctly which means that discriminant function parameters are correct.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7dXpRdsC0C6"
      },
      "source": [
        "### Sequential Widrow Hoff Learning Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyQVduhbC7xK"
      },
      "source": [
        "class SequentialWidrowHoff:\n",
        "    def fit(X, Y, a, b, n, iterations):\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Applying Sample Normalisation:\n",
        "        Norm_Y = []\n",
        "\n",
        "        for x, y in zip(X, Y):\n",
        "\n",
        "            # If the sample belongs to the class with label 2 or -1 (Check dataset in question to see how formatted):\n",
        "            if y == -1 or y == 2:\n",
        "                x = [i * -1 for i in x]\n",
        "                x.insert(0, y)\n",
        "                Norm_Y.append(x)\n",
        "            else:\n",
        "                x.insert(0, y)\n",
        "                Norm_Y.append(x)\n",
        "\n",
        "        print(\"Vectors used in Sequential Widrow-Hoff Learning Algorithm:\\n {}\\n\".format(Norm_Y))\n",
        "\n",
        "\n",
        "        # ------------------------------------------------------------------------------------\n",
        "        # Sequential Widrow-Hoff Learning Algorithm\n",
        "\n",
        "        # Epoch for-loop:\n",
        "        for o in range(int(iterations / len(Norm_Y))):\n",
        "\n",
        "            # This for-loop goes through each sample one-by-one:\n",
        "            for i in range(len(Norm_Y)):\n",
        "\n",
        "                # Value of a to use. If first iteration, then uses parameters given in question:\n",
        "\n",
        "                a_prev = a\n",
        "\n",
        "                # Which sample to use:\n",
        "                y_input = Norm_Y[i]\n",
        "                print(\"Sample used for this iteration is: {}\".format(y_input))\n",
        "\n",
        "                # Equation -> g(x) = a^{t}y\n",
        "                ay = np.dot(a, y_input)\n",
        "                print(\"g(x) = {}\".format(ay))\n",
        "\n",
        "                # Calculating the values for update:\n",
        "                update = np.zeros(len(y_input))\n",
        "                for j in range(len(y_input)): \n",
        "\n",
        "                    # Applying Update Rule of Sequential Widrow-Hoff Learning Algorithm:\n",
        "                    update[j] = n * (b[i] - ay) * y_input[j]\n",
        "\n",
        "                # Adding update to a:\n",
        "                a = np.add(a, update)\n",
        "                print(\"New Value of a^t is: {}\\n\".format(a))\n",
        "\n",
        "        print(\"Gone through all of the iterations as asked for in question.\")\n",
        "        \n",
        "    # def setParams():\n",
        "    #     X_train = transformToList(input(\"Enter features eg [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]] 2D:  \"))\n",
        "    #     y_train = transformToList(input(\"Enter labels eg [1, 1, 1, -1, -1, -1] 1D:  \"))\n",
        "    #     a = transformToList(input(\"Enter a. Usually [1, w1, w2...] eg [1, 0, 0] 1D:  \"))\n",
        "    #     b = transformToList(input(\"Enter b eg [1, 0.5, 1.5, 1.5, 1.5, 1] 1D:  \"))\n",
        "    #     lr = float(input(\"Enter the learning rate eg 0.1:  \"))\n",
        "    #     epochs = int(input(\"Enter the number of epochs eg 12:  \"))\n",
        "    #     return X_train, y_train, a, b, lr, epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK_OJ_jKDCnu",
        "outputId": "9238249a-aeb5-4510-c3d2-6c6ca047c107"
      },
      "source": [
        "X_train = [[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]\n",
        "y_train = [1,1,1,-1,-1,-1]\n",
        "a = [1,0,0]\n",
        "b = [1,1,1,1,1,1]\n",
        "lr = 0.1\n",
        "iterations = 12\n",
        "SequentialWidrowHoff.fit(X_train, y_train, a, b, lr, iterations)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectors used in Sequential Widrow-Hoff Learning Algorithm:\n",
            " [[1, 0, 2], [1, 1, 2], [1, 2, 1], [-1, 3, -1], [-1, 2, 1], [-1, 3, 2]]\n",
            "\n",
            "Sample used for this iteration is: [1, 0, 2]\n",
            "g(x) = 1\n",
            "New Value of a^t is: [1. 0. 0.]\n",
            "\n",
            "Sample used for this iteration is: [1, 1, 2]\n",
            "g(x) = 1.0\n",
            "New Value of a^t is: [1. 0. 0.]\n",
            "\n",
            "Sample used for this iteration is: [1, 2, 1]\n",
            "g(x) = 1.0\n",
            "New Value of a^t is: [1. 0. 0.]\n",
            "\n",
            "Sample used for this iteration is: [-1, 3, -1]\n",
            "g(x) = -1.0\n",
            "New Value of a^t is: [ 0.8  0.6 -0.2]\n",
            "\n",
            "Sample used for this iteration is: [-1, 2, 1]\n",
            "g(x) = 0.20000000000000012\n",
            "New Value of a^t is: [ 0.72  0.76 -0.12]\n",
            "\n",
            "Sample used for this iteration is: [-1, 3, 2]\n",
            "g(x) = 1.32\n",
            "New Value of a^t is: [ 0.752  0.664 -0.184]\n",
            "\n",
            "Sample used for this iteration is: [1, 0, 2]\n",
            "g(x) = 0.384\n",
            "New Value of a^t is: [ 0.8136  0.664  -0.0608]\n",
            "\n",
            "Sample used for this iteration is: [1, 1, 2]\n",
            "g(x) = 1.3559999999999999\n",
            "New Value of a^t is: [ 0.778   0.6284 -0.132 ]\n",
            "\n",
            "Sample used for this iteration is: [1, 2, 1]\n",
            "g(x) = 1.9028\n",
            "New Value of a^t is: [ 0.6877  0.4478 -0.2223]\n",
            "\n",
            "Sample used for this iteration is: [-1, 3, -1]\n",
            "g(x) = 0.8780799999999996\n",
            "New Value of a^t is: [ 0.6755  0.4844 -0.2345]\n",
            "\n",
            "Sample used for this iteration is: [-1, 2, 1]\n",
            "g(x) = 0.05883199999999983\n",
            "New Value of a^t is: [ 0.5814  0.6726 -0.1404]\n",
            "\n",
            "Sample used for this iteration is: [-1, 3, 2]\n",
            "g(x) = 1.1558272\n",
            "New Value of a^t is: [ 0.597   0.6259 -0.1715]\n",
            "\n",
            "Gone through all of the iterations as asked for in question.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO8rdBH4F42S"
      },
      "source": [
        "### K nearest neighbor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OaOab3SF7aB"
      },
      "source": [
        "def euc_dist (x1,x2,x3,x4):\n",
        "  euc_dist = math.sqrt( (x1-x3) ** 2 + (x2-x4) ** 2)\n",
        "  return euc_dist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_YoKRI1GEv0",
        "outputId": "e4d47abe-ac4f-4dc4-b8fe-2c18d1b58062"
      },
      "source": [
        "euc_dist(0.15,0.35,0.1,0.25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11180339887498945"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiMkG1EzOg05"
      },
      "source": [
        "## Week 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy0mI1UiOjKn"
      },
      "source": [
        "### Neuron Output (with heavy side function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHfzyKJON6of"
      },
      "source": [
        "class NeuronOutput:\n",
        "    def fit(weight, threshold, x):\n",
        "        summation = []\n",
        "        for i in range(len(x)):\n",
        "            summation.append(weight[i] * x[i])\n",
        "\n",
        "        summation = np.sum(summation, 0) - threshold\n",
        "\n",
        "        # Find output of neuron by applying heaviside function with given threshold:\n",
        "        output = np.heaviside(summation, threshold)\n",
        "        print(\"Output of neuron with input {} is {}.\".format(x, output))\n",
        "        \n",
        "    # def setParams():\n",
        "    #     #This script is based off of Question 2 in Tutorial 3\n",
        "    #     x = transformToList(input(\"Enter single sample/input eg [0.1, -0.5, 0.4] 1D:  \"))\n",
        "    #     weights = transformToList(input(\"Enter weigths from one layer to another eg [0.1, -5, 0.4] 1D:  \"))\n",
        "    #     threshold = float(input(\"Enter the threshold value eg 0:  \"))\n",
        "    #     return weights, threshold, x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjxkKoLuOpHA",
        "outputId": "2c573840-34cb-490c-e528-455dbbbcfe3c"
      },
      "source": [
        "weights = [0.1,-0.5,0.4]\n",
        "threshold = 0\n",
        "x = [0.1,-0.5, 0.4]\n",
        "NeuronOutput.fit(weights, threshold, x)\n",
        "\n",
        "x2 = [0.1,0.5,0.4]\n",
        "NeuronOutput.fit(weights, threshold, x2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output of neuron with input [0.1, -0.5, 0.4] is 1.0.\n",
            "Output of neuron with input [0.1, 0.5, 0.4] is 0.0.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_vLLFLzQeaS"
      },
      "source": [
        "### Sequential Delta Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZCf11RdQgtt"
      },
      "source": [
        "# Sequential Delta Learning Rule\n",
        "\n",
        "# INPUTS:\n",
        "    # x: data\n",
        "    # y: target\n",
        "    # w: initial weights\n",
        "    # eta: learning rate\n",
        "    # n_epochs: number of epochs\n",
        "    # augmentation: True if data is augmented\n",
        "    # sample_normalisation: True if sample normalisation applied\n",
        "    \n",
        "def sequential_delta_learning(x, y, w, eta, n_epochs, augmentation, sample_normalisation):\n",
        "    n_instances = len(x)\n",
        "    \n",
        "    if augmentation:\n",
        "        x = np.insert(x, 0, 1, axis=1)\n",
        "    \n",
        "    if sample_normalisation: # CAREFUL WITH THE DATA LABEL\n",
        "        x[y==-1] = -x[y==-1]\n",
        "        \n",
        "    for e in range(n_epochs):\n",
        "        print(\"Epoch #{}\".format(e+1))\n",
        "            \n",
        "        for i in range(n_instances):\n",
        "            print(\"Iteration #{}\".format(e*n_instances+i+1))\n",
        "\n",
        "            # y=H(Wx)\n",
        "            output1 =  np.dot(w, x[i]) # W*x\n",
        "            if output1 < 0: output=0 \n",
        "            elif output1 > 0: output=1\n",
        "            elif output1 == 0: output=0.5 # This means that for the Heaviside function we have H(0)=0.5\n",
        "            print(\"y=H(wx)=H({})={}\".format(output1, output))\n",
        "\n",
        "            # Now we update w if there is a difference between t (desired output==y[i]) and y\n",
        "            t_minus_y = y[i]-output\n",
        "            print ('t-y = ', t_minus_y)\n",
        "            eta_t_minus_y = eta*(y[i]-output)*x[i]\n",
        "            print ('eta_t_minus_y_xt = ', eta_t_minus_y)\n",
        "            w = w + eta*(y[i]-output)*x[i]\n",
        "            print(\"w_new={} \\n\".format(w))\n",
        "\n",
        "        print(\"End of epoch #{} \\n\".format(e+1))\n",
        "\n",
        "    return w      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHP_WirlSlbR",
        "outputId": "9cae722c-b7f2-4b49-d8ae-9395b49cb24b"
      },
      "source": [
        "# Tutorial example (exercise 3)\n",
        "x = np.array([[0], [1]]) # data\n",
        "y = np.array([1, 0]) # targets\n",
        "w = np.array([-1.5, 2]) # initial weights #THIS NEEDS TO BE FORMED FROM THE Q\n",
        "\n",
        "# And here we estimate the new vector\n",
        "w = sequential_delta_learning(x, y, w, 1, 6, True, False)\n",
        "print(w) # It should be [ 0.5 -1. ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Iteration #1\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "w_new=[-0.5  2. ] \n",
            "\n",
            "Iteration #2\n",
            "y=H(wx)=H(1.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-1.5  1. ] \n",
            "\n",
            "End of epoch #1 \n",
            "\n",
            "Epoch #2\n",
            "Iteration #3\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "w_new=[-0.5  1. ] \n",
            "\n",
            "Iteration #4\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-1.5  0. ] \n",
            "\n",
            "End of epoch #2 \n",
            "\n",
            "Epoch #3\n",
            "Iteration #5\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "w_new=[-0.5  0. ] \n",
            "\n",
            "Iteration #6\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[-0.5  0. ] \n",
            "\n",
            "End of epoch #3 \n",
            "\n",
            "Epoch #4\n",
            "Iteration #7\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "w_new=[0.5 0. ] \n",
            "\n",
            "Iteration #8\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-0.5 -1. ] \n",
            "\n",
            "End of epoch #4 \n",
            "\n",
            "Epoch #5\n",
            "Iteration #9\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "Iteration #10\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "End of epoch #5 \n",
            "\n",
            "Epoch #6\n",
            "Iteration #11\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "Iteration #12\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "End of epoch #6 \n",
            "\n",
            "[ 0.5 -1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhTiZ7-Zmhu"
      },
      "source": [
        "### Batch Delta Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zXV7dK0ZquX"
      },
      "source": [
        "# Batch Delta Learning Rule\n",
        "\n",
        "# INPUTS:\n",
        "    # x: data\n",
        "    # y: target\n",
        "    # w: initial weights\n",
        "    # eta: learning rate\n",
        "    # n_epochs: number of epochs\n",
        "    # augmentation: True if data is augmented\n",
        "    # sample_normalisation: True if sample normalisation applied\n",
        "    \n",
        "def batch_delta_learning(x, y, w, eta, n_epochs, augmentation, sample_normalisation):\n",
        "    n_instances = len(x)\n",
        "    \n",
        "    if augmentation:\n",
        "        x = np.insert(x, 0, 1, axis=1)\n",
        "    \n",
        "    if sample_normalisation: # CAREFUL WITH THE DATA LABEL\n",
        "        x[y==-1] = -x[y==-1]\n",
        "        \n",
        "    for e in range(n_epochs):\n",
        "        print(\"Epoch #{}\".format(e+1))\n",
        "        sumando = 0\n",
        "            \n",
        "        for i in range(n_instances):\n",
        "            print(\"Iteration #{}\".format(e*n_instances+i+1))\n",
        "\n",
        "            # y=H(Wx)\n",
        "            output1 =  np.dot(w, x[i]) # W*x\n",
        "            if output1 < 0: output=0 \n",
        "            elif output1 > 0: output=1\n",
        "            elif output1 == 0: output=0.5 # This means that for the Heaviside function we have H(0)=0.5\n",
        "            print(\"y=H(wx)=H({})={}\".format(output1, output))\n",
        "\n",
        "            t_minus_y = y[i]-output\n",
        "            print ('t-y = ', t_minus_y)\n",
        "            eta_t_minus_y = eta*(y[i]-output)*x[i]\n",
        "            print ('eta_t_minus_y_xt = ', eta_t_minus_y)\n",
        "            \n",
        "            sumando += (y[i]-output)*x[i]\n",
        "\n",
        "        # Now we update w if there is a difference between t (desired output==y[i]) and y\n",
        "        w = w + eta*sumando\n",
        "        print(\"w_new={} \\n\".format(w))\n",
        "\n",
        "        print(\"End of epoch #{} \\n\".format(e+1))\n",
        "\n",
        "    return w   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG-95609Zy56",
        "outputId": "54a0c798-0b3e-425a-facd-e16400590798"
      },
      "source": [
        "# Tutorial example (exercise 4)\n",
        "x = np.array([[0], [1]]) # data\n",
        "y = np.array([1, 0]) # targets\n",
        "w = np.array([-1.5, 2]) # initial weights\n",
        "\n",
        "# And here we estimate the new vector\n",
        "w = batch_delta_learning(x, y, w, 1, 7, True, False)\n",
        "print(w) # It should be [ 0.5 -1. ]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Iteration #1\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "Iteration #2\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-1.5  1. ] \n",
            "\n",
            "End of epoch #1 \n",
            "\n",
            "Epoch #2\n",
            "Iteration #3\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "Iteration #4\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[-0.5  1. ] \n",
            "\n",
            "End of epoch #2 \n",
            "\n",
            "Epoch #3\n",
            "Iteration #5\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "Iteration #6\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-0.5  0. ] \n",
            "\n",
            "End of epoch #3 \n",
            "\n",
            "Epoch #4\n",
            "Iteration #7\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "Iteration #8\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[0.5 0. ] \n",
            "\n",
            "End of epoch #4 \n",
            "\n",
            "Epoch #5\n",
            "Iteration #9\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "Iteration #10\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1]\n",
            "w_new=[-0.5 -1. ] \n",
            "\n",
            "End of epoch #5 \n",
            "\n",
            "Epoch #6\n",
            "Iteration #11\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0]\n",
            "Iteration #12\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "End of epoch #6 \n",
            "\n",
            "Epoch #7\n",
            "Iteration #13\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "Iteration #14\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0]\n",
            "w_new=[ 0.5 -1. ] \n",
            "\n",
            "End of epoch #7 \n",
            "\n",
            "[ 0.5 -1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2G9iAuA1Z6tR",
        "outputId": "63efed94-a7d7-4a6e-acd2-ea5d4aa93f65"
      },
      "source": [
        "# Tutorial example (exercise 5)\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]) # data\n",
        "y = np.array([0, 0, 0, 1]) # targets\n",
        "w = np.array([0.5, 1, 1]) # initial weights # REMEMBER TO GIVE minus theta here\n",
        "\n",
        "# And here we estimate the new vector\n",
        "w = sequential_delta_learning(x, y, w, 1, 5, True, False)\n",
        "print(w) # It should be [-2.5  2.   1. ]  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Iteration #1\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1  0  0]\n",
            "w_new=[-0.5  1.   1. ] \n",
            "\n",
            "Iteration #2\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1  0 -1]\n",
            "w_new=[-1.5  1.   0. ] \n",
            "\n",
            "Iteration #3\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-1.5  1.   0. ] \n",
            "\n",
            "Iteration #4\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 1 1]\n",
            "w_new=[-0.5  2.   1. ] \n",
            "\n",
            "End of epoch #1 \n",
            "\n",
            "Epoch #2\n",
            "Iteration #5\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-0.5  2.   1. ] \n",
            "\n",
            "Iteration #6\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1  0 -1]\n",
            "w_new=[-1.5  2.   0. ] \n",
            "\n",
            "Iteration #7\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1  0]\n",
            "w_new=[-2.5  1.   0. ] \n",
            "\n",
            "Iteration #8\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 1 1]\n",
            "w_new=[-1.5  2.   1. ] \n",
            "\n",
            "End of epoch #2 \n",
            "\n",
            "Epoch #3\n",
            "Iteration #9\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-1.5  2.   1. ] \n",
            "\n",
            "Iteration #10\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-1.5  2.   1. ] \n",
            "\n",
            "Iteration #11\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1 -1  0]\n",
            "w_new=[-2.5  1.   1. ] \n",
            "\n",
            "Iteration #12\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 1 1]\n",
            "w_new=[-1.5  2.   2. ] \n",
            "\n",
            "End of epoch #3 \n",
            "\n",
            "Epoch #4\n",
            "Iteration #13\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-1.5  2.   2. ] \n",
            "\n",
            "Iteration #14\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1  0 -1]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "Iteration #15\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "Iteration #16\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "End of epoch #4 \n",
            "\n",
            "Epoch #5\n",
            "Iteration #17\n",
            "y=H(wx)=H(-2.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "Iteration #18\n",
            "y=H(wx)=H(-1.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "Iteration #19\n",
            "y=H(wx)=H(-0.5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "Iteration #20\n",
            "y=H(wx)=H(0.5)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[-2.5  2.   1. ] \n",
            "\n",
            "End of epoch #5 \n",
            "\n",
            "[-2.5  2.   1. ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPncan9GclJj",
        "outputId": "1e06e33a-2309-4855-cae3-503f653be64e"
      },
      "source": [
        "# Tutorial example (exercise 6)\n",
        "x = np.array([[0, 2], [1, 2], [2, 1], [-3, 1], [-2, -1], [-3, -2]]) # data\n",
        "y = np.array([1, 1, 1, 0, 0, 0]) # targets\n",
        "w = np.array([1, 0, 0]) # initial weights\n",
        "\n",
        "# And here we estimate the new vector\n",
        "w = sequential_delta_learning(x, y, w, 1, 3, True, False)\n",
        "print(w) # It should be [1 3 1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Iteration #1\n",
            "y=H(wx)=H(1)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 0 0] \n",
            "\n",
            "Iteration #2\n",
            "y=H(wx)=H(1)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 0 0] \n",
            "\n",
            "Iteration #3\n",
            "y=H(wx)=H(1)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 0 0] \n",
            "\n",
            "Iteration #4\n",
            "y=H(wx)=H(1)=1\n",
            "t-y =  -1\n",
            "eta_t_minus_y_xt =  [-1  3 -1]\n",
            "w_new=[ 0  3 -1] \n",
            "\n",
            "Iteration #5\n",
            "y=H(wx)=H(-5)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[ 0  3 -1] \n",
            "\n",
            "Iteration #6\n",
            "y=H(wx)=H(-7)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[ 0  3 -1] \n",
            "\n",
            "End of epoch #1 \n",
            "\n",
            "Epoch #2\n",
            "Iteration #7\n",
            "y=H(wx)=H(-2)=0\n",
            "t-y =  1\n",
            "eta_t_minus_y_xt =  [1 0 2]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #8\n",
            "y=H(wx)=H(6)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #9\n",
            "y=H(wx)=H(8)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #10\n",
            "y=H(wx)=H(-7)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #11\n",
            "y=H(wx)=H(-6)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #12\n",
            "y=H(wx)=H(-10)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "End of epoch #2 \n",
            "\n",
            "Epoch #3\n",
            "Iteration #13\n",
            "y=H(wx)=H(3)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #14\n",
            "y=H(wx)=H(6)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #15\n",
            "y=H(wx)=H(8)=1\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #16\n",
            "y=H(wx)=H(-7)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #17\n",
            "y=H(wx)=H(-6)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "Iteration #18\n",
            "y=H(wx)=H(-10)=0\n",
            "t-y =  0\n",
            "eta_t_minus_y_xt =  [0 0 0]\n",
            "w_new=[1 3 1] \n",
            "\n",
            "End of epoch #3 \n",
            "\n",
            "[1 3 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaYQ7ZcDgGH0"
      },
      "source": [
        "### Negative Feedback Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGhu8sI4dUVm"
      },
      "source": [
        "class NegativeFeddbackNetwork:\n",
        "    def fit(weights, iterations, x, alpha, activations):\n",
        "        prev_activations = activations\n",
        "        iteration = 1\n",
        "\n",
        "        for i in range(iterations):\n",
        "\n",
        "            print(\"Iteration {}\".format(iteration))\n",
        "\n",
        "            # Following block deals with calculating first equation: e = x - W^{T}y\n",
        "            wT = np.array(weights).T\n",
        "            wTy = np.dot(wT, activations)\n",
        "            print(\"value of wTy {}\".format(wTy))\n",
        "\n",
        "            eT = x - wTy\n",
        "            print(\"eT: {}\".format(eT))\n",
        "            e = np.array(eT).reshape((eT.shape[0], 1))\n",
        "\n",
        "            # The following lines deal with calculating the update: y <- y + \\alpha*W*e\n",
        "            We = np.dot(weights, e)\n",
        "            We = [j for i in We for j in i]\n",
        "            print(\"We: {} \".format(We))\n",
        "\n",
        "            alphaWe = np.dot(alpha, We)\n",
        "\n",
        "            # Doing the actual update using the second equation:\n",
        "            y = activations + alphaWe\n",
        "            print(\"Value of y: {}\".format(y))\n",
        "\n",
        "            activations = y\n",
        "\n",
        "            \n",
        "            wTy = np.dot(wT, activations)\n",
        "            print(\"value of wTy with new y{}\\n\".format(wTy))\n",
        "            iteration += 1\n",
        "\n",
        "        print(\"\\nAfter {} iterations, the activation of the output neurons is equal to {}\".format(iterations, activations))\n",
        "        \n",
        "    # def setParams():\n",
        "    #     print(\"This script is based off of Question 7 in Tutorial 3\")\n",
        "    #     x = transformToList(input(\"Enter single sample/input eg [1, 1, 0] 1D:  \"))\n",
        "    #     weights = transformToList(input(\"Enter weigths from one layer to another eg [[1, 1, 0], [1, 1, 1]] 2D:  \"))\n",
        "    #     activation = transformToList(input(\"Enter output layer initial activation eg [0, 0] 1D:  \"))\n",
        "    #     lr = float(input(\"Enter the learning rate/alpha eg 0.25.  \"))\n",
        "    #     epochs = int(input(\"Enter the number of epochs eg 5.  \"))\n",
        "    #     return weights, epochs, x, lr, activation\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4vxpOdYe7L-",
        "outputId": "f2026fb8-d5e7-4a56-c719-f31bf3eb7a03"
      },
      "source": [
        "weights = [[1,1,0],[1,1,1]]\n",
        "epochs = 5\n",
        "x = [1,1,0]\n",
        "lr = 0.25\n",
        "activation = [0,0]\n",
        "NegativeFeddbackNetwork.fit(weights, epochs, x, lr, activation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "value of wTy [0 0 0]\n",
            "eT: [1 1 0]\n",
            "We: [2, 2] \n",
            "Value of y: [0.5 0.5]\n",
            "value of wTy with new y[1.  1.  0.5]\n",
            "\n",
            "Iteration 2\n",
            "value of wTy [1.  1.  0.5]\n",
            "eT: [ 0.   0.  -0.5]\n",
            "We: [0.0, -0.5] \n",
            "Value of y: [0.5   0.375]\n",
            "value of wTy with new y[0.875 0.875 0.375]\n",
            "\n",
            "Iteration 3\n",
            "value of wTy [0.875 0.875 0.375]\n",
            "eT: [ 0.125  0.125 -0.375]\n",
            "We: [0.25, -0.125] \n",
            "Value of y: [0.5625 0.3438]\n",
            "value of wTy with new y[0.9062 0.9062 0.3438]\n",
            "\n",
            "Iteration 4\n",
            "value of wTy [0.9062 0.9062 0.3438]\n",
            "eT: [ 0.0938  0.0938 -0.3438]\n",
            "We: [0.1875, -0.15625] \n",
            "Value of y: [0.6094 0.3047]\n",
            "value of wTy with new y[0.9141 0.9141 0.3047]\n",
            "\n",
            "Iteration 5\n",
            "value of wTy [0.9141 0.9141 0.3047]\n",
            "eT: [ 0.0859  0.0859 -0.3047]\n",
            "We: [0.171875, -0.1328125] \n",
            "Value of y: [0.6523 0.2715]\n",
            "value of wTy with new y[0.9238 0.9238 0.2715]\n",
            "\n",
            "\n",
            "After 5 iterations, the activation of the output neurons is equal to [0.6523 0.2715]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL7rPP1pfm-l",
        "outputId": "8121abd2-7aa9-4f0a-b7a9-29cbf4d03c93"
      },
      "source": [
        "weights = [[1,1,0],[1,1,1]]\n",
        "epochs = 5\n",
        "x = [1,1,0]\n",
        "lr = 0.5\n",
        "activation = [0,0]\n",
        "NegativeFeddbackNetwork.fit(weights, epochs, x, lr, activation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1\n",
            "value of wTy [0 0 0]\n",
            "eT: [1 1 0]\n",
            "We: [2, 2] \n",
            "Value of y: [1. 1.]\n",
            "value of wTy with new y[2. 2. 1.]\n",
            "\n",
            "Iteration 2\n",
            "value of wTy [2. 2. 1.]\n",
            "eT: [-1. -1. -1.]\n",
            "We: [-2.0, -3.0] \n",
            "Value of y: [ 0.  -0.5]\n",
            "value of wTy with new y[-0.5 -0.5 -0.5]\n",
            "\n",
            "Iteration 3\n",
            "value of wTy [-0.5 -0.5 -0.5]\n",
            "eT: [1.5 1.5 0.5]\n",
            "We: [3.0, 3.5] \n",
            "Value of y: [1.5  1.25]\n",
            "value of wTy with new y[2.75 2.75 1.25]\n",
            "\n",
            "Iteration 4\n",
            "value of wTy [2.75 2.75 1.25]\n",
            "eT: [-1.75 -1.75 -1.25]\n",
            "We: [-3.5, -4.75] \n",
            "Value of y: [-0.25  -1.125]\n",
            "value of wTy with new y[-1.375 -1.375 -1.125]\n",
            "\n",
            "Iteration 5\n",
            "value of wTy [-1.375 -1.375 -1.125]\n",
            "eT: [2.375 2.375 1.125]\n",
            "We: [4.75, 5.875] \n",
            "Value of y: [2.125  1.8125]\n",
            "value of wTy with new y[3.9375 3.9375 1.8125]\n",
            "\n",
            "\n",
            "After 5 iterations, the activation of the output neurons is equal to [2.125  1.8125]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0SUlGRff2x7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XusIcmEDg4mC"
      },
      "source": [
        "### REGULATORY FEEDBACK // DIVISIVE INPUT MODULATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_qm13YzhBvF"
      },
      "source": [
        "# Regulatory Feedback // Divisive Input Modulation\n",
        "\n",
        "# INPUTS:\n",
        "    # x: input weights\n",
        "    # y: output weights\n",
        "    # w: initial weights\n",
        "    # epsilon: parameters\n",
        "    # n_epochs: number of epochs\n",
        "    # augmentation: True if data is augmented\n",
        "    # sample_normalisation: True if sample normalisation applied\n",
        "\n",
        "def regulatory_feedforward_network(x, y, w, epsilon, n_epochs, augmentation, sample_normalisation):\n",
        "    n_instances = len(x)\n",
        "    \n",
        "    if augmentation:\n",
        "        x = np.insert(x, 0, 1, axis=1)\n",
        "    \n",
        "    if sample_normalisation: # CAREFUL WITH THE DATA LABEL\n",
        "        x[y==-1] = -x[y==-1]\n",
        "    \n",
        "    w_hat = np.zeros(w.shape)\n",
        "    for i in range(len(w)):\n",
        "        w_hat[i] = w[i]/w[i].sum()\n",
        "        \n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch #{}\".format(epoch+1))\n",
        "            \n",
        "        for i in range(n_instances):\n",
        "            print(\"Iteration #{}\".format(epoch*n_instances+i+1))\n",
        "\n",
        "            # [Wt*y]epsilon2\n",
        "            result = np.dot(w.T, y)\n",
        "            for j in range(len(result)):\n",
        "                if result[j] < epsilon[1]: result[j]=epsilon[1]\n",
        "            \n",
        "            # e = x - wT*y\n",
        "            e = x[i]/result\n",
        "            \n",
        "            # [y]epsilon1\n",
        "            new_y = np.zeros(y.shape)\n",
        "            for j in range(len(y)):\n",
        "                if y[j] < epsilon[0]: new_y[j]=epsilon[0]\n",
        "                else: new_y[j]=y[j]\n",
        "            \n",
        "            # y = new_y * [w_hat*e]\n",
        "            y = new_y*np.dot(w_hat, e)       \n",
        "\n",
        "            print(\"eT = {}\\n(W_hat*e)T = {}\\n yT = {}\\n (WTy)T = {}\".format(e, np.dot(w_hat, e), y, np.dot(w.T, y)))\n",
        "\n",
        "        print(\"End of epoch #{} \\n\".format(epoch+1))\n",
        "\n",
        "    return e, y, w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BWtHqZJhGZU",
        "outputId": "e4772b02-5bca-443f-9633-e479a827b79d"
      },
      "source": [
        "# Tutorial example (exercise 9)\n",
        "x = np.array([[1., 1., 0.]]) # inputs\n",
        "y = np.array([0., 0.]) # outputs\n",
        "w = np.array([[1., 1., 0.], [1., 1., 1.]]) # weights\n",
        "epsilon = np.array([0.01, 0.01])\n",
        "\n",
        "# And here we estimate the new values\n",
        "e, y, w = regulatory_feedforward_network(x, y, w, epsilon, 5, False, False)\n",
        "print(\"e: {}\\ny: {}\\nw: {}\".format(e, y, w)) # It should be y: [0.83505  0.10997]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch #1\n",
            "Iteration #1\n",
            "eT = [100. 100.   0.]\n",
            "(W_hat*e)T = [100.      66.6667]\n",
            " yT = [1.     0.6667]\n",
            " (WTy)T = [1.6667 1.6667 0.6667]\n",
            "End of epoch #1 \n",
            "\n",
            "Epoch #2\n",
            "Iteration #2\n",
            "eT = [0.6 0.6 0. ]\n",
            "(W_hat*e)T = [0.6 0.4]\n",
            " yT = [0.6    0.2667]\n",
            " (WTy)T = [0.8667 0.8667 0.2667]\n",
            "End of epoch #2 \n",
            "\n",
            "Epoch #3\n",
            "Iteration #3\n",
            "eT = [1.1538 1.1538 0.    ]\n",
            "(W_hat*e)T = [1.1538 0.7692]\n",
            " yT = [0.6923 0.2051]\n",
            " (WTy)T = [0.8974 0.8974 0.2051]\n",
            "End of epoch #3 \n",
            "\n",
            "Epoch #4\n",
            "Iteration #4\n",
            "eT = [1.1143 1.1143 0.    ]\n",
            "(W_hat*e)T = [1.1143 0.7429]\n",
            " yT = [0.7714 0.1524]\n",
            " (WTy)T = [0.9238 0.9238 0.1524]\n",
            "End of epoch #4 \n",
            "\n",
            "Epoch #5\n",
            "Iteration #5\n",
            "eT = [1.0825 1.0825 0.    ]\n",
            "(W_hat*e)T = [1.0825 0.7216]\n",
            " yT = [0.8351 0.11  ]\n",
            " (WTy)T = [0.945 0.945 0.11 ]\n",
            "End of epoch #5 \n",
            "\n",
            "e: [1.0825 1.0825 0.    ]\n",
            "y: [0.8351 0.11  ]\n",
            "w: [[1. 1. 0.]\n",
            " [1. 1. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENrJMLpuog3b"
      },
      "source": [
        "## Week 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJKROtcHoTpp",
        "outputId": "831bc4bd-4cb0-4c25-800b-e0ae7f9d4ba1"
      },
      "source": [
        "# tanh\n",
        "round (math.tanh(1), 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZAnlrafpFGQ"
      },
      "source": [
        "### Batch Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVE67MCMpG9Q"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=8)\n",
        "\n",
        "def batch_normalization(batch, beta, gamma, epsilon):\n",
        "    mean = np.zeros(batch[0].shape)\n",
        "    for X in batch:\n",
        "        mean = mean + X\n",
        "    mean = mean / len(batch)\n",
        "\n",
        "    variance = np.zeros(batch[0].shape)\n",
        "    for X in batch:\n",
        "        variance = variance + (X - mean) ** 2\n",
        "    variance = variance / len(batch)\n",
        "\n",
        "    batch_normalized = list()\n",
        "    for X in batch:\n",
        "        batch_normalized.append(beta + gamma * (X - mean) / np.sqrt(variance + epsilon))\n",
        "\n",
        "    return batch_normalized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzmQse3wpM8O",
        "outputId": "d0834f6d-d5e8-4a5e-8bfb-e5ed56625593"
      },
      "source": [
        "X1 = np.array([[1, 0.5, 0.2], [-1, -0.5, -0.2], [0.1, -0.1, 0]])\n",
        "X2 = np.array([[1, -1, 0.1], [0.5, -0.5, -0.1], [0.2, -0.2, 0]])\n",
        "X3 = np.array([[0.5, -0.5, -0.1], [0, -0.4, 0], [0.5, 0.5, 0.2]])\n",
        "X4 = np.array([[0.2, 1, -0.2], [-1, -0.6, -0.1], [0.1, 0, 0.1]])\n",
        "beta = 0\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "\n",
        "# ---------------------\n",
        "for i, a in enumerate(batch_normalization((X1, X2, X3, X4), beta, gamma, epsilon)):\n",
        "    print(f\"X{i+1}:\\n {a}\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1:\n",
            " [[ 0.69787657  0.58722022  0.56568542]\n",
            " [-0.86516068  0.         -0.3086067 ]\n",
            " [-0.3509312  -0.36115756 -0.22941573]]\n",
            "\n",
            "X2:\n",
            " [[ 0.69787657 -1.17444044  0.28284271]\n",
            " [ 1.21122495  0.          0.        ]\n",
            " [-0.07018624 -0.60192927 -0.22941573]]\n",
            "\n",
            "X3:\n",
            " [[-0.37577969 -0.58722022 -0.28284271]\n",
            " [ 0.51909641  0.3086067   0.3086067 ]\n",
            " [ 0.77204865  1.08347268  0.38235956]]\n",
            "\n",
            "X4:\n",
            " [[-1.01997344  1.17444044 -0.56568542]\n",
            " [-0.86516068 -0.3086067   0.        ]\n",
            " [-0.3509312  -0.12038585  0.07647191]]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9m3s_aupl58"
      },
      "source": [
        "### Convolution Layer (padding, stride, dilation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALntTLjmpobO"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=8)\n",
        "\n",
        "def convolution_layer(feature_maps, mask_channels, padding, stride, dilation):\n",
        "    nh, nw = feature_maps[0].shape\n",
        "    mh, mw = mask_channels[0].shape\n",
        "    nh_out, nw_out = int((nh + 2 * padding - mh ) / stride) + 1 - (dilation - 1), int((nw + 2 * padding - mw ) / stride) + 1 - (dilation - 1)\n",
        "    out = np.zeros((nh_out, nw_out))\n",
        "    for i in range(nh_out):\n",
        "        for j in range(nw_out):\n",
        "            for X, H in zip(feature_maps, mask_channels):\n",
        "                X = np.pad(X, padding)\n",
        "                if stride != 0:\n",
        "                    i_start = i * stride\n",
        "                    j_start = j * stride\n",
        "                else:\n",
        "                    i_start = i\n",
        "                    j_start = j\n",
        "                i_end = i_start + mh * dilation - dilation + 1\n",
        "                j_end = j_start + mw * dilation - dilation + 1\n",
        "                out[i, j] = out[i, j] + np.sum(X[i_start:i_end:dilation, j_start:j_end:dilation] * H)\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VDVJwRRqVws",
        "outputId": "808fc40b-3f0b-4f94-ae31-a8254e5b5af6"
      },
      "source": [
        "# Initialise your matrices that needs to go through the convolutional layer\n",
        "feature_maps = list()\n",
        "feature_maps.append(np.array([[1.0, 0, 2], [0.4,-0.7,0.7], [0.6,-0.6,0.6]]))\n",
        "feature_maps.append(np.array([[1, -1, 0.6], [0.6, -0.5,0.7], [1,-0.4,0.7]]))\n",
        "\n",
        "# Initialise the masks\n",
        "mask_channels = list()\n",
        "mask_channels.append(np.array([[7.2,1.5], [-0.8,4.9]]))\n",
        "mask_channels.append(np.array([[3.1,-9.5], [1.7,7.7]]))\n",
        "\n",
        "#Set the desired padding, stride, and dilation\n",
        "padding = 1 # NOTE padding should be set to 0 if not provided\n",
        "stride = 2 # NOTE stride should be set to 1 if not provided\n",
        "dilation = 1 # NOTE dilation should be set to 1 if not provided\n",
        "\n",
        "#----------------------------------------------------------------------------#\n",
        "convolution_layer(feature_maps, mask_channels, padding=padding, stride=stride, dilation=dilation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[12.6 , 12.72],\n",
              "       [ 5.54, -4.06]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ5qV9terFZ7"
      },
      "source": [
        "###  Convolution Layer (pooling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCpQYdFtrF-J"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(suppress=True, precision=8)\n",
        "\n",
        "def pooling_layer(feature_maps, pool_size, stride, pooling='max'):\n",
        "    nh, nw = feature_maps[0].shape\n",
        "    nh_out, nw_out = int((nh - pool_size[0]) / stride) + 1, int((nw - pool_size[1]) / stride) + 1\n",
        "    out_maps = list()\n",
        "    for X in feature_maps:\n",
        "        out = np.zeros((nh_out, nw_out))\n",
        "        for i in range(nh_out):\n",
        "            for j in range(nw_out):\n",
        "                i_start = i * stride\n",
        "                j_start = j * stride\n",
        "                i_end = i_start + pool_size[0]\n",
        "                j_end = j_start + pool_size[1]\n",
        "                if pooling == 'avg':\n",
        "                    out[i, j] = np.average(X[i_start:i_end, j_start:j_end])\n",
        "                else:\n",
        "                    out[i, j] = np.max(X[i_start:i_end, j_start:j_end])\n",
        "        out_maps.append(out)\n",
        "    return(out_maps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zSJ5j4xrIeu",
        "outputId": "c4b71b36-f290-4c24-c955-b014e409b2b5"
      },
      "source": [
        "\n",
        "# Enter the matrix to apply pooling on.\n",
        "feature_maps = list()\n",
        "feature_maps.append(np.array([[0.2, 1, 0, 0.4], [-1, 0, -0.1, -0.1], [0.1, 0, -1, -0.5], [0.4, -0.7, -0.5, 1]]))\n",
        "\n",
        "#Set the desired stride and pooling\n",
        "stride = 2 # NOTE stride should be set to 1 if not provided\n",
        "pooling = 'max' # It can be either 'avg' or 'max'\n",
        "\n",
        "# Set the dimension of the pooling\n",
        "x1 = 2\n",
        "x2 = 2 # This is a 2x2 pooling\n",
        "\n",
        "#-------------------------------------------------------------------#\n",
        "output = pooling_layer(feature_maps, (x1, x2), stride=stride, pooling=pooling)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[1. , 0.4],\n",
            "       [0.4, 1. ]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2n47q8frqvF"
      },
      "source": [
        "### Convolution Layer - output size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zjNDxQUrrN8"
      },
      "source": [
        "def get_output_size(input_size, mask_size, padding, stride, dilation=1):\n",
        "    (nh, nw, nc) = input_size\n",
        "    (mh, mw, mc) = mask_size\n",
        "    nh_out, nw_out = int((nh + 2 * padding - mh) / stride) + 1 - (dilation - 1), int(\n",
        "        (nw + 2 * padding - mw) / stride) + 1 - (dilation - 1)\n",
        "    return (nh_out, nw_out, mc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rF2RCPrrvwH",
        "outputId": "572661c7-96e4-474f-d402-29dc45729e89"
      },
      "source": [
        "# NOTE This only works for single masks and not for concatenations of CNN. For that, look at the image below\n",
        "\n",
        "\n",
        "input_size = (11, 15, 6) # hight x width x feature maps \n",
        "mask_size = (3, 3, 1) # hight x width x channels\n",
        "\n",
        "output_size = get_output_size(input_size, mask_size, padding=0, stride=2)\n",
        "print(output_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 7, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8v61-S5rxoT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBUDuQe0vm36"
      },
      "source": [
        "## Week 7 - Feature Extraction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRaYzszHwXv0"
      },
      "source": [
        "###  Karhunen-Loève Transform PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CjUSyGxwYRA"
      },
      "source": [
        "# Karhunen-Loève Transform (KLT) -> For PCA \n",
        "\n",
        "# INPUT:\n",
        "    # data: pd.DataFrame with instances as rows and values as columns \n",
        "\n",
        "def karhunen_pd(data):\n",
        "    \n",
        "    # First we calculate the mean of each column\n",
        "    means = np.zeros(len(data.columns))# for all columns\n",
        "    for i in range(len(data.columns)):\n",
        "        means[i] = data.iloc[:, i].mean()\n",
        "    \n",
        "    mean_zero = np.zeros(data.shape)\n",
        "    # Now we substract the corresponding mean to all the values of our dataset\n",
        "    for j in range(len(data)): # for all instances\n",
        "        mean_zero[j, :] = data.iloc[j, :] - means\n",
        "        \n",
        "    # Now we have to calculate the covariance matrix\n",
        "    cov = np.zeros((len(data.columns), len(data.columns)))\n",
        "    for i in range(len(data)): # for all instances\n",
        "        cov += np.array([mean_zero[i, :]]).T*mean_zero[i, :]\n",
        "    cov = cov/len(data) # This is our final covariance matrix\n",
        "    \n",
        "    # Lastly, we obtain the eigenvalues and its corresponding eigenvectors\n",
        "    eigenVal, eigenVec = np.linalg.eig(cov)\n",
        "    \n",
        "    return means, cov, eigenVal, eigenVec\n",
        "\n",
        "# Karhunen-Loève Transform (KLT) -> For PCA \n",
        "\n",
        "# INPUT:\n",
        "    # data: np.array with instances as rows and values ascolumns \n",
        "\n",
        "def karhunen_np(data):\n",
        "    \n",
        "    # First we calculate the mean of each row\n",
        "    means = x.mean(axis=1)\n",
        "    \n",
        "    mean_zero = np.zeros(data.shape)\n",
        "    # Now we substract the corresponding mean to all the values of our dataset\n",
        "    for j in range(data.shape[1]): # for all instances\n",
        "        mean_zero[:, j] = data[:, j] - means\n",
        "        \n",
        "    # Now we have to calculate the covariance matrix\n",
        "    cov = np.zeros((data.shape[0],data.shape[0]))\n",
        "    for i in range(x.shape[1]): # for all instances\n",
        "        cov += np.array([mean_zero[:, i]]).T*mean_zero[:, i]\n",
        "    cov = cov/x.shape[1] # This is our final covariance matrix\n",
        "    \n",
        "    # Lastly, we obtain the eigenvalues and its corresponding eigenvectors\n",
        "    eigenVal, eigenVec = np.linalg.eig(cov)\n",
        "    \n",
        "    return means, mean_zero, cov, eigenVal, eigenVec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGSSyOOdXiHR",
        "outputId": "2615f632-f72c-4842-f318-5ce3692aa2df"
      },
      "source": [
        "# Tutorial example (exercise 4)\n",
        "x = np.array([[1., 2., 1.], [2, 3., 1.], [3., 5., 1.], [2., 2., 1.]]).T # input data\n",
        "\n",
        "means, mean_zero, cov, eigenVal, eigenVec = karhunen_np(x)\n",
        "\n",
        "print(\"Means: {}\\n\\nCovariance Matrix: \\n{}\\n\\nEigenValues: {}\\n\\nEigenVectors: \\n{}\".format(means, cov, eigenVal, eigenVec))\n",
        "\n",
        "# We choose the two largest:\n",
        "val = np.array([eigenVal[1], eigenVal[0]])\n",
        "v = np.array([eigenVec[:, 1], eigenVec[:, 0]]).T\n",
        "print(\"\\n\\nEigenValues for the 2 first principal components: {}\\n\\nEigenVectors for the 2 first principal components: \\n{}\".format(val, v))\n",
        "\n",
        "# Now, for a point you use: \n",
        "y = np.dot(v.T, mean_zero)\n",
        "print(\"\\n\\nPoint before PCA: \\n{}\\n\\nPoint with mean zero: \\n{}\\n\\nProjection into the two first principal components: \\n{}\".format(np.array(x), mean_zero, y))\n",
        "\n",
        "# Now we have to calculate the covariance matrix\n",
        "cov = np.zeros((y.shape[0],y.shape[0]))\n",
        "for i in range(y.shape[1]): # for all instances\n",
        "    cov += np.array([y[:, i]]).T*y[:, i]\n",
        "cov = cov/y.shape[1] # This is our final covariance matrix\n",
        "print(\"\\nNew covariance matrix: \\n{}\".format(cov))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Means: [2. 3. 1.]\n",
            "\n",
            "Covariance Matrix: \n",
            "[[0.5  0.75 0.  ]\n",
            " [0.75 1.5  0.  ]\n",
            " [0.   0.   0.  ]]\n",
            "\n",
            "EigenValues: [0.0986 1.9014 0.    ]\n",
            "\n",
            "EigenVectors: \n",
            "[[-0.8817 -0.4719  0.    ]\n",
            " [ 0.4719 -0.8817  0.    ]\n",
            " [ 0.      0.      1.    ]]\n",
            "\n",
            "\n",
            "EigenValues for the 2 first principal components: [1.9014 0.0986]\n",
            "\n",
            "EigenVectors for the 2 first principal components: \n",
            "[[-0.4719 -0.8817]\n",
            " [-0.8817  0.4719]\n",
            " [ 0.      0.    ]]\n",
            "\n",
            "\n",
            "Point before PCA: \n",
            "[[1. 2. 3. 2.]\n",
            " [2. 3. 5. 2.]\n",
            " [1. 1. 1. 1.]]\n",
            "\n",
            "Point with mean zero: \n",
            "[[-1.  0.  1.  0.]\n",
            " [-1.  0.  2. -1.]\n",
            " [ 0.  0.  0.  0.]]\n",
            "\n",
            "Projection into the two first principal components: \n",
            "[[ 1.3535  0.     -2.2352  0.8817]\n",
            " [ 0.4098  0.      0.062  -0.4719]]\n",
            "\n",
            "New covariance matrix: \n",
            "[[ 1.9014 -0.    ]\n",
            " [-0.      0.0986]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcSWjssyXrd1",
        "outputId": "fa7dd841-aa3a-4b7f-b032-0eb963e13e82"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Tutorial example (exercise 6)\n",
        "x = pd.DataFrame([[0., 1.], [3., 5.], [5., 4.], [5., 6.], [8., 7.], [9., 7.]]) # input data\n",
        "\n",
        "means, cov, eigenVal, eigenVec = karhunen_pd(x)\n",
        "\n",
        "print(\"Means: {}\\n\\nCovariance Matrix: \\n{}\\n\\nEigenValues: {}\\n\\nEigenVectors: \\n{}\".format(means, cov, eigenVal, eigenVec))\n",
        "\n",
        "print(\"\\nPoints with zero mean: \\n{}\".format(np.array(x-means)))\n",
        "\n",
        "# We choose the two largest:\n",
        "val = np.array(eigenVal[0])\n",
        "v = np.array(eigenVec[:, 0]).T\n",
        "print(\"\\n\\nEigenValues for the first principal component: {}\\n\\nEigenVectors for the first principal component: \\n{}\".format(val, v))\n",
        "\n",
        "# Now, for a point you use: \n",
        "y = np.dot(v.T, (x - means).T)\n",
        "print(\"\\n\\nPoint before PCA: \\n{}\\n\\nPoint with mean zero: \\n{}\\n\\nProjection into the two first principal components: \\n{}\".format(np.array(x), np.array(x - means), y))\n",
        "\n",
        "# FASTER WAY TO DO IT WITH SCIKIT LEARN\n",
        "pca=PCA(n_components=2)\n",
        "ytest=pca.fit(x).transform(np.array(x)) # for the first point\n",
        "print(\"\\n\\nNew Points: \\n{}\".format(ytest.T))\n",
        "\n",
        "# New covariance matrix for this dataset\n",
        "print(\"\\n\\nCovariance matrix of new dataset: \\n{}\".format(np.cov(y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Means: [5. 5.]\n",
            "\n",
            "Covariance Matrix: \n",
            "[[9.     5.6667]\n",
            " [5.6667 4.3333]]\n",
            "\n",
            "EigenValues: [12.7949  0.5384]\n",
            "\n",
            "EigenVectors: \n",
            "[[ 0.8309 -0.5564]\n",
            " [ 0.5564  0.8309]]\n",
            "\n",
            "Points with zero mean: \n",
            "[[-5. -4.]\n",
            " [-2.  0.]\n",
            " [ 0. -1.]\n",
            " [ 0.  1.]\n",
            " [ 3.  2.]\n",
            " [ 4.  2.]]\n",
            "\n",
            "\n",
            "EigenValues for the first principal component: 12.79492543695008\n",
            "\n",
            "EigenVectors for the first principal component: \n",
            "[0.8309 0.5564]\n",
            "\n",
            "\n",
            "Point before PCA: \n",
            "[[0. 1.]\n",
            " [3. 5.]\n",
            " [5. 4.]\n",
            " [5. 6.]\n",
            " [8. 7.]\n",
            " [9. 7.]]\n",
            "\n",
            "Point with mean zero: \n",
            "[[-5. -4.]\n",
            " [-2.  0.]\n",
            " [ 0. -1.]\n",
            " [ 0.  1.]\n",
            " [ 3.  2.]\n",
            " [ 4.  2.]]\n",
            "\n",
            "Projection into the two first principal components: \n",
            "[-6.3802 -1.6618 -0.5564  0.5564  3.6055  4.4364]\n",
            "\n",
            "\n",
            "New Points: \n",
            "[[ 6.3802  1.6618  0.5564 -0.5564 -3.6055 -4.4364]\n",
            " [-0.5414  1.1129 -0.8309  0.8309 -0.0075 -0.564 ]]\n",
            "\n",
            "\n",
            "Covariance matrix of new dataset: \n",
            "15.353910524340101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUeF7sFarSS"
      },
      "source": [
        "### Oja's rule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nu_lSuRBYZ2r",
        "outputId": "74e8a100-0193-4aaf-a358-8f4328933f8c"
      },
      "source": [
        "# Oja's Learning rule \n",
        "# through Mizan \n",
        "# this code works out zero-mean data so please enter the raw feature vectors as given in the question\n",
        "# Tutorial 7, question 7\n",
        "\n",
        "## -----------------------------------------------------------------------\n",
        "# ONLY CHANGE THESE 3 INPUTS AND CHANGE THE NUMBER OF EPOCH YOU WANT WHEN APPLYING THE FUNCTION(Oja_learning_rule)\n",
        "input_from_question = np.array([[[0,1]],[[3,5]],[[5,4]],[[5,6]],[[8,7]],[[9,7]]])\n",
        "weight_x = np.array([[-1,0]])\n",
        "learning_rate = 0.01\n",
        "epochs = 6\n",
        "## -----------------------------------------------------------------------\n",
        "\n",
        "# we are going to perform Oja's learning on the input_vectors\n",
        "input_vectors = []\n",
        "\n",
        "mean_of_data = input_from_question.mean(axis=0)\n",
        "for i in input_from_question:\n",
        "    zero_mean_data = i - mean_of_data\n",
        "    input_vectors.append(zero_mean_data)\n",
        "    \n",
        "\n",
        "def Oja_learning_rule(epoch):\n",
        "    weight_update = np.copy(weight_x)  \n",
        "    for i in range(1,epoch+1):\n",
        "        df = pd.DataFrame({\"x\": [i for i in input_vectors]})\n",
        "        df['y'] = df['x'].apply(lambda x: np.dot(x,weight_update.T))\n",
        "        df['x - yw'] = df['x'].apply(lambda x: np.round(x, 4)) - df['y'].apply(lambda y: y * (weight_update))  \n",
        "        df['ny(x -yw)'] = learning_rate * df['y'].apply(lambda y: y) * df['x - yw'].apply(lambda x: x)\n",
        "        #Rounding the numbers         \n",
        "        df['y'] = df['y'].apply(lambda y: np.round(y,4))\n",
        "        df['x - yw'] =  df['x - yw'].apply(lambda x: np.round(x,4))\n",
        "        df['ny(x -yw)']  = df['ny(x -yw)'].apply(lambda x: np.round(x,4))\n",
        "        sum_of_weights = df['ny(x -yw)'].sum()\n",
        "        weight_update = weight_update + sum_of_weights   \n",
        "        display(df)\n",
        "        print(f'after {i} epoch Total weight change is: {sum_of_weights}')\n",
        "        print(f'after {i} epoch our weights are: {weight_update}')\n",
        "\n",
        "\n",
        "Oja_learning_rule(epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[5.0]]</td>\n",
              "      <td>[[0.0, -4.0]]</td>\n",
              "      <td>[[0.0, -0.2]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[2.0]]</td>\n",
              "      <td>[[0.0, 0.0]]</td>\n",
              "      <td>[[0.0, 0.0]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.0]]</td>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.0, -0.0]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[0.0]]</td>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[0.0, 0.0]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.0]]</td>\n",
              "      <td>[[0.0, 2.0]]</td>\n",
              "      <td>[[-0.0, -0.06]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.0]]</td>\n",
              "      <td>[[0.0, 2.0]]</td>\n",
              "      <td>[[-0.0, -0.08]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x         y         x - yw        ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[5.0]]  [[0.0, -4.0]]    [[0.0, -0.2]]\n",
              "1   [[-2.0, 0.0]]   [[2.0]]   [[0.0, 0.0]]     [[0.0, 0.0]]\n",
              "2   [[0.0, -1.0]]   [[0.0]]  [[0.0, -1.0]]    [[0.0, -0.0]]\n",
              "3    [[0.0, 1.0]]   [[0.0]]   [[0.0, 1.0]]     [[0.0, 0.0]]\n",
              "4    [[3.0, 2.0]]  [[-3.0]]   [[0.0, 2.0]]  [[-0.0, -0.06]]\n",
              "5    [[4.0, 2.0]]  [[-4.0]]   [[0.0, 2.0]]  [[-0.0, -0.08]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 1 epoch Total weight change is: [[ 0.   -0.34]]\n",
            "after 1 epoch our weights are: [[-1.   -0.34]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[6.36]]</td>\n",
              "      <td>[[1.36, -1.8376]]</td>\n",
              "      <td>[[0.0865, -0.1169]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[2.0]]</td>\n",
              "      <td>[[0.0, 0.68]]</td>\n",
              "      <td>[[0.0, 0.0136]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.34]]</td>\n",
              "      <td>[[0.34, -0.8844]]</td>\n",
              "      <td>[[0.0012, -0.003]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[-0.34]]</td>\n",
              "      <td>[[-0.34, 0.8844]]</td>\n",
              "      <td>[[0.0012, -0.003]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.68]]</td>\n",
              "      <td>[[-0.68, 0.7488]]</td>\n",
              "      <td>[[0.025, -0.0276]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.68]]</td>\n",
              "      <td>[[-0.68, 0.4088]]</td>\n",
              "      <td>[[0.0318, -0.0191]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x          y             x - yw            ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[6.36]]  [[1.36, -1.8376]]  [[0.0865, -0.1169]]\n",
              "1   [[-2.0, 0.0]]    [[2.0]]      [[0.0, 0.68]]      [[0.0, 0.0136]]\n",
              "2   [[0.0, -1.0]]   [[0.34]]  [[0.34, -0.8844]]   [[0.0012, -0.003]]\n",
              "3    [[0.0, 1.0]]  [[-0.34]]  [[-0.34, 0.8844]]   [[0.0012, -0.003]]\n",
              "4    [[3.0, 2.0]]  [[-3.68]]  [[-0.68, 0.7488]]   [[0.025, -0.0276]]\n",
              "5    [[4.0, 2.0]]  [[-4.68]]  [[-0.68, 0.4088]]  [[0.0318, -0.0191]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 2 epoch Total weight change is: [[ 0.1457 -0.156 ]]\n",
            "after 2 epoch our weights are: [[-0.8543 -0.496 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[6.2555]]</td>\n",
              "      <td>[[0.3441, -0.8973]]</td>\n",
              "      <td>[[0.0215, -0.0561]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[1.7086]]</td>\n",
              "      <td>[[-0.5403, 0.8475]]</td>\n",
              "      <td>[[-0.0092, 0.0145]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.496]]</td>\n",
              "      <td>[[0.4237, -0.754]]</td>\n",
              "      <td>[[0.0021, -0.0037]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[-0.496]]</td>\n",
              "      <td>[[-0.4237, 0.754]]</td>\n",
              "      <td>[[0.0021, -0.0037]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.5549]]</td>\n",
              "      <td>[[-0.037, 0.2368]]</td>\n",
              "      <td>[[0.0013, -0.0084]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.4092]]</td>\n",
              "      <td>[[0.2332, -0.187]]</td>\n",
              "      <td>[[-0.0103, 0.0082]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x            y               x - yw            ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[6.2555]]  [[0.3441, -0.8973]]  [[0.0215, -0.0561]]\n",
              "1   [[-2.0, 0.0]]   [[1.7086]]  [[-0.5403, 0.8475]]  [[-0.0092, 0.0145]]\n",
              "2   [[0.0, -1.0]]    [[0.496]]   [[0.4237, -0.754]]  [[0.0021, -0.0037]]\n",
              "3    [[0.0, 1.0]]   [[-0.496]]   [[-0.4237, 0.754]]  [[0.0021, -0.0037]]\n",
              "4    [[3.0, 2.0]]  [[-3.5549]]   [[-0.037, 0.2368]]  [[0.0013, -0.0084]]\n",
              "5    [[4.0, 2.0]]  [[-4.4092]]   [[0.2332, -0.187]]  [[-0.0103, 0.0082]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 3 epoch Total weight change is: [[ 0.0075 -0.0492]]\n",
            "after 3 epoch our weights are: [[-0.8468 -0.5452]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[6.4148]]</td>\n",
              "      <td>[[0.4321, -0.5027]]</td>\n",
              "      <td>[[0.0277, -0.0322]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[1.6936]]</td>\n",
              "      <td>[[-0.5659, 0.9234]]</td>\n",
              "      <td>[[-0.0096, 0.0156]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.5452]]</td>\n",
              "      <td>[[0.4617, -0.7028]]</td>\n",
              "      <td>[[0.0025, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[-0.5452]]</td>\n",
              "      <td>[[-0.4617, 0.7028]]</td>\n",
              "      <td>[[0.0025, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.6308]]</td>\n",
              "      <td>[[-0.0746, 0.0205]]</td>\n",
              "      <td>[[0.0027, -0.0007]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.4776]]</td>\n",
              "      <td>[[0.2084, -0.4412]]</td>\n",
              "      <td>[[-0.0093, 0.0198]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x            y               x - yw            ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[6.4148]]  [[0.4321, -0.5027]]  [[0.0277, -0.0322]]\n",
              "1   [[-2.0, 0.0]]   [[1.6936]]  [[-0.5659, 0.9234]]  [[-0.0096, 0.0156]]\n",
              "2   [[0.0, -1.0]]   [[0.5452]]  [[0.4617, -0.7028]]  [[0.0025, -0.0038]]\n",
              "3    [[0.0, 1.0]]  [[-0.5452]]  [[-0.4617, 0.7028]]  [[0.0025, -0.0038]]\n",
              "4    [[3.0, 2.0]]  [[-3.6308]]  [[-0.0746, 0.0205]]  [[0.0027, -0.0007]]\n",
              "5    [[4.0, 2.0]]  [[-4.4776]]  [[0.2084, -0.4412]]  [[-0.0093, 0.0198]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 4 epoch Total weight change is: [[ 0.0165 -0.0051]]\n",
            "after 4 epoch our weights are: [[-0.8303 -0.5503]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[6.3527]]</td>\n",
              "      <td>[[0.2746, -0.5041]]</td>\n",
              "      <td>[[0.0174, -0.032]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[1.6606]]</td>\n",
              "      <td>[[-0.6212, 0.9138]]</td>\n",
              "      <td>[[-0.0103, 0.0152]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.5503]]</td>\n",
              "      <td>[[0.4569, -0.6972]]</td>\n",
              "      <td>[[0.0025, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[-0.5503]]</td>\n",
              "      <td>[[-0.4569, 0.6972]]</td>\n",
              "      <td>[[0.0025, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.5915]]</td>\n",
              "      <td>[[0.018, 0.0236]]</td>\n",
              "      <td>[[-0.0006, -0.0008]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.4218]]</td>\n",
              "      <td>[[0.3286, -0.4333]]</td>\n",
              "      <td>[[-0.0145, 0.0192]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x            y               x - yw             ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[6.3527]]  [[0.2746, -0.5041]]    [[0.0174, -0.032]]\n",
              "1   [[-2.0, 0.0]]   [[1.6606]]  [[-0.6212, 0.9138]]   [[-0.0103, 0.0152]]\n",
              "2   [[0.0, -1.0]]   [[0.5503]]  [[0.4569, -0.6972]]   [[0.0025, -0.0038]]\n",
              "3    [[0.0, 1.0]]  [[-0.5503]]  [[-0.4569, 0.6972]]   [[0.0025, -0.0038]]\n",
              "4    [[3.0, 2.0]]  [[-3.5915]]    [[0.018, 0.0236]]  [[-0.0006, -0.0008]]\n",
              "5    [[4.0, 2.0]]  [[-4.4218]]  [[0.3286, -0.4333]]   [[-0.0145, 0.0192]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 5 epoch Total weight change is: [[-0.003 -0.006]]\n",
            "after 5 epoch our weights are: [[-0.8333 -0.5563]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>x - yw</th>\n",
              "      <th>ny(x -yw)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-5.0, -4.0]]</td>\n",
              "      <td>[[6.3917]]</td>\n",
              "      <td>[[0.3262, -0.4443]]</td>\n",
              "      <td>[[0.0208, -0.0284]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-2.0, 0.0]]</td>\n",
              "      <td>[[1.6666]]</td>\n",
              "      <td>[[-0.6112, 0.9271]]</td>\n",
              "      <td>[[-0.0102, 0.0155]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[0.0, -1.0]]</td>\n",
              "      <td>[[0.5563]]</td>\n",
              "      <td>[[0.4636, -0.6905]]</td>\n",
              "      <td>[[0.0026, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[0.0, 1.0]]</td>\n",
              "      <td>[[-0.5563]]</td>\n",
              "      <td>[[-0.4636, 0.6905]]</td>\n",
              "      <td>[[0.0026, -0.0038]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[3.0, 2.0]]</td>\n",
              "      <td>[[-3.6125]]</td>\n",
              "      <td>[[-0.0103, -0.0096]]</td>\n",
              "      <td>[[0.0004, 0.0003]]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>[[4.0, 2.0]]</td>\n",
              "      <td>[[-4.4458]]</td>\n",
              "      <td>[[0.2953, -0.4732]]</td>\n",
              "      <td>[[-0.0131, 0.021]]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                x            y                x - yw            ny(x -yw)\n",
              "0  [[-5.0, -4.0]]   [[6.3917]]   [[0.3262, -0.4443]]  [[0.0208, -0.0284]]\n",
              "1   [[-2.0, 0.0]]   [[1.6666]]   [[-0.6112, 0.9271]]  [[-0.0102, 0.0155]]\n",
              "2   [[0.0, -1.0]]   [[0.5563]]   [[0.4636, -0.6905]]  [[0.0026, -0.0038]]\n",
              "3    [[0.0, 1.0]]  [[-0.5563]]   [[-0.4636, 0.6905]]  [[0.0026, -0.0038]]\n",
              "4    [[3.0, 2.0]]  [[-3.6125]]  [[-0.0103, -0.0096]]   [[0.0004, 0.0003]]\n",
              "5    [[4.0, 2.0]]  [[-4.4458]]   [[0.2953, -0.4732]]   [[-0.0131, 0.021]]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "after 6 epoch Total weight change is: [[0.0031 0.0008]]\n",
            "after 6 epoch our weights are: [[-0.8302 -0.5555]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhK9snIWaYsh"
      },
      "source": [
        "### Fisher's method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCNhz3xPZLFM"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def fishers(ip, weights, classes):\n",
        "    ip = np.array(ip)\n",
        "    N, D = ip.shape\n",
        "    weights = np.array(weights)\n",
        "    m1 = []\n",
        "    m2 = []\n",
        "    for idx in range(N):\n",
        "        if classes[idx] == 1:\n",
        "            m1.append(ip[idx])\n",
        "        else:\n",
        "            m2.append(ip[idx])\n",
        "    m1 = np.mean(m1, axis=0)\n",
        "    m2 = np.mean(m2, axis=0)\n",
        "\n",
        "    # between cluster distance\n",
        "    sb = []\n",
        "    sw = []\n",
        "    for w in (weights):\n",
        "        d = (w @ (m1-m2)) ** 2\n",
        "        sb.append(d)\n",
        "    # calculate within cluster distance\n",
        "    sw = []\n",
        "    for w in weights:\n",
        "        running_sw = 0\n",
        "        for idx in range(len(ip)):\n",
        "            if classes[idx] == 1:\n",
        "                running_sw += (w.T @ (ip[idx] - m1)) ** 2\n",
        "\n",
        "            elif classes[idx] == 2:\n",
        "                running_sw += (w.T @ (ip[idx] - m2)) ** 2\n",
        "        sw.append(running_sw)\n",
        "        # print(running_sw)\n",
        "    print(\"SB: \")\n",
        "    print(sb)\n",
        "    print(\"SW: \")\n",
        "    print(sw)\n",
        "    cost = []\n",
        "    for _sb, _sw in zip(sb, sw):\n",
        "        cost.append(_sb/_sw)\n",
        "    print(\"Cost: \")\n",
        "    print(cost)\n",
        "\n",
        "    print(\"-\"*100)\n",
        "    print(f\"{weights[np.argmax(cost)]} has high PROJECTION COST\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z88ApotZadSL",
        "outputId": "ac479345-c41c-474b-a3e4-7dae96a90d70"
      },
      "source": [
        "ip = [[1, 2], [2, 1], [3, 3], [6, 5], [7, 8]]\n",
        "classes = [1, 1, 1, 2, 2]\n",
        "weights = [[-1, 5], [2, -3]]\n",
        "fishers(ip, weights, classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SB: \n",
            "[324.0, 20.25]\n",
            "SW: \n",
            "[140.0, 38.5]\n",
            "Cost: \n",
            "[2.3142857142857145, 0.525974025974026]\n",
            "----------------------------------------------------------------------------------------------------\n",
            "[-1  5] has high PROJECTION COST\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhxqBEuEbq9L"
      },
      "source": [
        "### Sparse coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rLbAyf3a9a3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def main(p, VT, x, _lambda):\n",
        "    p = np.array(p)\n",
        "    VT = np.array(VT)\n",
        "    x = np.array(x)\n",
        "    r_error = []\n",
        "    for p in projections:\n",
        "        val = x - VT @ p\n",
        "        r_error.append(np.linalg.norm(val) + _lambda*np.count_nonzero(p))\n",
        "    print(\"RECONSTRUCTION ERRORS: \")\n",
        "    print(r_error)\n",
        "    print(projections[np.argmin(r_error)], \" for sparse coding\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYB9MwGDbt2e",
        "outputId": "714bf426-c68c-49e7-8583-cf157c90f365"
      },
      "source": [
        "# REPLACE ACCORDING TO THE QUESTION\n",
        "# projections are nothing but y\n",
        "\n",
        "projections = [[0,-1.4,0,0], [1.6,-1.0,0,0]]\n",
        "x = [[2.4, 4.3]]\n",
        "VT = [[1.6, -0.1, 4.6, -1.9], [1.4,-2.9,-0.7,8.3]]\n",
        "main(p=projections, VT=VT, x=x, _lambda=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RECONSTRUCTION ERRORS: \n",
            "[3.272707636278806, 2.8793179174792245]\n",
            "[1.6, -1.0, 0, 0]  for sparse coding\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIrb6YZJcRAe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiKSaTsCc0hj"
      },
      "source": [
        "## Week 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMlH0Jiwc37q"
      },
      "source": [
        "### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2tjOOpRc2hf"
      },
      "source": [
        "class SVM:\n",
        "    def fit(X, y, support_vectors, support_vector_class):\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "\n",
        "        print(\"-\"*100)\n",
        "        w = []\n",
        "        for idx in range(len(support_vectors)):\n",
        "            w.append(support_vectors[idx] * support_vector_class[idx])\n",
        "        w = np.array(w)\n",
        "        eq_arr = []\n",
        "        for idx, sv in enumerate(support_vectors):\n",
        "            tmp = ((w @ sv) * support_vector_class[idx])\n",
        "            tmp = np.append(tmp, [support_vector_class[idx]])\n",
        "            eq_arr.append(tmp)\n",
        "        eq_arr.append(np.append(support_vector_class, [0]))\n",
        "        rhs_arr = [1] * len(support_vector_class)\n",
        "        rhs_arr.extend([0])\n",
        "        rhs_arr = np.array(rhs_arr)\n",
        "        ans = rhs_arr @ np.linalg.inv(eq_arr)\n",
        "        print(\"lambda and w_0 values are \", ans)\n",
        "        final_weight = []\n",
        "        for idx in range(w.shape[0]):\n",
        "            final_weight.append(w[idx] * ans[idx])\n",
        "        final_weight = np.array(final_weight)\n",
        "        final_weight = np.sum(final_weight, axis=0)\n",
        "        print(\"Weights: \")\n",
        "        print(final_weight)\n",
        "        print(\"Margin: \")\n",
        "        print(2/np.linalg.norm(final_weight))\n",
        "        print(\"-\"*100)\n",
        "            \n",
        "    def setParams():\n",
        "        X = transformToList(input(\"Enter features eg [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]] 2D:  \"))\n",
        "        y = transformToList(input(\"Enter labels eg [1, 1, 1, 1, -1, -1, -1, -1] 1D:  \"))\n",
        "        support_vectors = np.array(transformToList(input(\"Enter features eg [[3, 1], [3, -1], [1, 0]] 2D:  \"))).astype(np.float)\n",
        "        support_vector_class = np.array(transformToList(input(\"Enter labels eg [1, 1, -1] 1D:  \"))).astype(np.float)\n",
        "            \n",
        "        return X, y, support_vectors, support_vector_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "oWvIIv7zc9Rc",
        "outputId": "2d8955d5-8ecc-4ff5-950c-0a5c4ecd1b56"
      },
      "source": [
        "X, y, support_vectors, support_vector_class = SVM.setParams()\n",
        "SVM.fit(X, y, support_vectors, support_vector_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter features eg [[3, 1], [3, -1], [7, 1], [8, 0], [1, 0], [0, 1], [-1, 0], [-2, 0]] 2D:  [1]\n",
            "Enter labels eg [1, 1, 1, 1, -1, -1, -1, -1] 1D:  [-1,-1,+1]\n",
            "Enter features eg [[3, 1], [3, -1], [1, 0]] 2D:  [6,9,8]\n",
            "Enter labels eg [1, 1, -1] 1D:  [-1,-1,+1]\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-2fb67dbe0347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vector_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport_vector_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-2412b9b1bb2b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(X, y, support_vectors, support_vector_class)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0meq_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msupport_vector_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msupport_vector_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0meq_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3zF-VLYgWYl"
      },
      "source": [
        "## Week 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl9FMmJEgdrq"
      },
      "source": [
        "### AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hya7lpjDgafC"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "# NOTE you can use if for Q1 c in tutorial9\n",
        "\n",
        "def run_adaBooster_setup():\n",
        "    \"\"\" Ask the user to enter the needed parameters\n",
        "    \"\"\"  \n",
        "    print(\"Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \\nBe careful not to enter spaces after the coma that separates the samples' coordinates.\\n\")\n",
        "    print(\"I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \\n\")\n",
        "    dataset_input = str(input())\n",
        "    dataset = np.array([]).reshape(0,2)\n",
        "    samples = dataset_input.split(' ')\n",
        "    for sample in samples:\n",
        "        coordinates = sample.split(',')\n",
        "        coordinates = np.array([float(c) for c in coordinates])\n",
        "        dataset = np.concatenate((dataset, [coordinates]))\n",
        "    print(\"\\n\")\n",
        "    print(\"Please insert the labels (y) of each sample in the dataset. \\nYou should provide a sequential list of y where each y is separated by a single space.\")\n",
        "    print(\"I.E. +1 -1 +1 -1\")\n",
        "    labels = [ int(o) for o in str(input()).split(' ')] \n",
        "    print(\"\\n\")\n",
        "    print(\"Please insert the number of weak classifiers:\")\n",
        "    n_classifiers = int(input())\n",
        "    print(\"\\n\")\n",
        "    thresholds = np.array([])\n",
        "    for i in range(0, n_classifiers):\n",
        "        print(f\"Please insert the decision threshold for weak classifier {i+1} so that it classify a sample to be +1:\")\n",
        "        print(\"I.E.: x1 > 0\")\n",
        "        print(\"I.E.: x2 > 3\")\n",
        "        print(\"I.E.: x1 >= -4\")\n",
        "        thresholds = np.concatenate((thresholds, [str(input())]))\n",
        "        print('\\n')\n",
        "    print(\"Please enter the target training error (when the adaboost should terminate). This should be normalised to the total number of samples:\")\n",
        "    print(\"I.E.: 0 -> if you want to stop it when the classifier classifies correctly all the training samples.\")\n",
        "    print(\"I.E.: 0.25 -> if you want to stop it when the classifier classifies correctly 75\\% of the samples.\")\n",
        "    target_error = float(input())\n",
        "    print(\"\\n\")\n",
        "    print(\"Please enter the maximum number of iterations you want the algorithm to run for:\")\n",
        "    max_iterations = int(input())\n",
        "    print(\"\\n\")\n",
        "    return dataset, labels, n_classifiers, thresholds, target_error, max_iterations\n",
        "\n",
        "\n",
        "# Decision stump used as weak classifier\n",
        "class DecisionStump():\n",
        "    def __init__(self, id, threshold=None):\n",
        "        self.threshold = threshold\n",
        "        self.id = id\n",
        "\n",
        "\n",
        "    def predict(self, sample):\n",
        "        if '>' in self.threshold:\n",
        "            terms = self.threshold.split(' ')\n",
        "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
        "            if sample[axis_in_condition[0]] > float(terms[2]):\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "        elif '<' in self.threshold:\n",
        "            terms = self.threshold.split(' ')\n",
        "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
        "            if sample[axis_in_condition[0]] < float(terms[2]):\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "        elif '<=' in self.threshold:\n",
        "            terms = self.threshold.split(' ')\n",
        "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
        "            if sample[axis_in_condition[0]] <= float(terms[2]):\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "        elif '>=' in self.threshold:\n",
        "            terms = self.threshold.split(' ')\n",
        "            axis_in_condition = [int(c) - 1 for c in terms[0] if c.isdigit()]\n",
        "            if sample[axis_in_condition[0]] >= float(terms[2]):\n",
        "                return 1\n",
        "            else:\n",
        "                return -1\n",
        "\n",
        "\n",
        "class Adaboost():\n",
        "\n",
        "    def __init__(self, n_clf, thresholds, target_error, max_iterations=10):\n",
        "        self.n_clf = n_clf\n",
        "        self.clfs = np.array([])\n",
        "        for i in range (0, len(thresholds)):\n",
        "            self.clfs = np.concatenate(( self.clfs, [DecisionStump(i+1, thresholds[i])] ))\n",
        "        # self.alpha = 0\n",
        "        self.alpha = []\n",
        "        self.target_error = target_error\n",
        "        self.max_iterations = max_iterations\n",
        "        self.best_classifiers = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Initialize weights to 1/N\n",
        "        w = np.full(n_samples, (1 / n_samples))\n",
        "\n",
        "        iteration = 1\n",
        "        while True:\n",
        "            print(f\"Iteration {iteration}: \\nWeights: {w}\")\n",
        "\n",
        "            # Iterate through classifiers and find the best one\n",
        "            lowest_error = 100000000\n",
        "            best_classifier = 0\n",
        "            for i in range(0, self.n_clf):\n",
        "                clf = self.clfs[i]\n",
        "                # Calculate Error\n",
        "                err = 0\n",
        "                for j, sample in enumerate(X):\n",
        "                    prediction = clf.predict(sample)\n",
        "                    error = 0 if prediction == y[j] else 1\n",
        "                    err += error * w[j]\n",
        "                if err < lowest_error:\n",
        "                    lowest_error = err\n",
        "                    best_classifier = i\n",
        "\n",
        "            self.best_classifiers.append(self.clfs[best_classifier])\n",
        "            print(f\"Best classifier: {best_classifier+1}\")\n",
        "\n",
        "            # Get predictions from best classifer for each sample\n",
        "            predictions = []\n",
        "            for j, sample in enumerate(X):\n",
        "                prediction = self.clfs[best_classifier].predict(sample)\n",
        "                predictions.append(prediction)\n",
        "\n",
        "            # Calculate weighted training error of best classifier\n",
        "            weighted_error = 0\n",
        "            for i, prediction in enumerate(predictions):\n",
        "                # error += 0 if prediction == y[i] else 1\n",
        "                w_e = 0 if prediction == y[i] else 1\n",
        "                w_e *= w[i]\n",
        "                weighted_error += w_e\n",
        "            error /= len(y)\n",
        "            print(f\"Best classifier's weighted training error: {weighted_error}\")\n",
        "\n",
        "\n",
        "            # Calculate Alpha\n",
        "            EPS = 1e-10\n",
        "            alpha = 0.5 * np.log((1.0 - lowest_error) / (lowest_error))\n",
        "            self.alpha.append(alpha)\n",
        "            print(f\"Alpha: {alpha}\")\n",
        "\n",
        "            # Calculate weights for next iteration\n",
        "            new_w = []\n",
        "            for i, weight in enumerate(w):\n",
        "                new_w.append(w[i] * (np.exp(- alpha * y[i] * predictions[i])))\n",
        "                print(f\"Update weight: W{iteration}(sample{i+1})*e^-alpha{iteration}*y{i+1}*h{iteration}(sample{i+1}) ----> {w[i] * (np.exp(- alpha * y[i] * predictions[i]))}\")\n",
        "            # Normalize to one\n",
        "            Z_normalisation = 0\n",
        "            for i, weight in enumerate(new_w):\n",
        "                Z_normalisation += weight\n",
        "            for i, weight in enumerate(new_w):\n",
        "                new_w[i] /= Z_normalisation\n",
        "            print(f\"Normalisation Z{iteration} when updating new weights: {Z_normalisation}\")\n",
        "            # Update weights for next iteration\n",
        "            w = new_w\n",
        "\n",
        "\n",
        "\n",
        "            # Check if the classifier has reached the desired target error\n",
        "            # Find the output*alpha of each classifier for each sample\n",
        "            tot_error = 0\n",
        "            decision_formula = ''\n",
        "            sample_classifications = np.zeros((X.shape[0], len(self.alpha)))\n",
        "            for i, alpha in enumerate(self.alpha):  \n",
        "                clf = self.best_classifiers[i]\n",
        "                for j, sample in enumerate(X):\n",
        "                    prediction = clf.predict(sample)\n",
        "                    sample_classifications[j][i] = alpha if prediction == y[j] else -alpha\n",
        "                decision_formula += f\"{alpha} * h{clf.id}(x) + \"\n",
        "            # Calculate the AdaBooster classification error in this round\n",
        "            sample_classifications = sample_classifications.sum(axis=1)\n",
        "            for i, classification in enumerate(sample_classifications):\n",
        "                classification = 1 if classification >= 0 else -1\n",
        "                tot_error += 1/X.shape[0] if classification == y[j] else 0\n",
        "            print(f\"AdaBoost Classifier in this round: {decision_formula[:-2]}\")\n",
        "            print(f\"AdaBoost Classifier (unweighted) error in this round: {tot_error}\")\n",
        "            # If the error is below our target error stop the execution\n",
        "            if tot_error <= self.target_error:\n",
        "                print('\\n')\n",
        "                print(f\"The final hard classifier is: sgn({decision_formula[:-2]})\")\n",
        "                return\n",
        "\n",
        "            # If we have reached the max iterations stop the execution\n",
        "            if iteration >= self.max_iterations:\n",
        "                return\n",
        "            iteration += 1\n",
        "            print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, sample):\n",
        "        tot_error = 0\n",
        "        sample_classifications = np.zeros((1, len(self.alpha)))\n",
        "        for i, alpha in enumerate(self.alpha):  \n",
        "            clf = self.best_classifiers[i]\n",
        "            prediction = clf.predict(sample)\n",
        "            sample_classifications[0][i] = alpha if prediction == 1 else -alpha\n",
        "        # Calculate the AdaBooster classification error in this round\n",
        "        sample_classifications = sample_classifications.sum(axis=1)\n",
        "        return 1 if sample_classifications[0] >= 0 else -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hiYX5k5igv7z",
        "outputId": "402aa4f7-9724-4c5a-896c-491b3b18b9ab"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    dataset, labels, n_classifiers, thresholds, target_error, max_iterations = run_adaBooster_setup()\n",
        "    classifier = Adaboost(n_classifiers, thresholds, target_error, max_iterations)\n",
        "    classifier.fit(dataset, labels)\n",
        "\n",
        "    while True:\n",
        "        print(\"\\nEnter a new sample (separating its coordinates with a coma and not including spaces) to predict or simply enter quit()\")\n",
        "        sample = str(input())\n",
        "        if sample == 'quit()':\n",
        "            quit()\n",
        "        result = classifier.predict([float(c) for c in sample.split(',')])\n",
        "        print(f\"The AdaBoost classifier classified it as {result}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please insert the dataset. Each sample should be divided by a space and each coordinate within a sample should be divided by a coma. \n",
            "Be careful not to enter spaces after the coma that separates the samples' coordinates.\n",
            "\n",
            "I.E.: x1=[1,2], x2=[-3,4], x3=[5,3] would be ---> 1,2 -3,4 5,3 \n",
            "\n",
            "1,0 -1,0 0,1 0,-1\n",
            "\n",
            "\n",
            "Please insert the labels (y) of each sample in the dataset. \n",
            "You should provide a sequential list of y where each y is separated by a single space.\n",
            "I.E. +1 -1 +1 -1\n",
            "+1 +1 -1 -1\n",
            "\n",
            "\n",
            "Please insert the number of weak classifiers:\n",
            "8\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 1 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x1 > -0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 2 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x2 <= -0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 3 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x3 > 0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 4 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x4 > 0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 5 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x5 > -0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 6 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x6 <= -0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 7 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x7 > 0.5\n",
            "\n",
            "\n",
            "Please insert the decision threshold for weak classifier 8 so that it classify a sample to be +1:\n",
            "I.E.: x1 > 0\n",
            "I.E.: x2 > 3\n",
            "I.E.: x1 >= -4\n",
            "x8 <= 0.5\n",
            "\n",
            "\n",
            "Please enter the target training error (when the adaboost should terminate). This should be normalised to the total number of samples:\n",
            "I.E.: 0 -> if you want to stop it when the classifier classifies correctly all the training samples.\n",
            "I.E.: 0.25 -> if you want to stop it when the classifier classifies correctly 75\\% of the samples.\n",
            "0\n",
            "\n",
            "\n",
            "Please enter the maximum number of iterations you want the algorithm to run for:\n",
            "4\n",
            "\n",
            "\n",
            "Iteration 1: \n",
            "Weights: [0.25 0.25 0.25 0.25]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9d3dbf781b1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_adaBooster_setup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaboost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classifiers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8e5a6f0f6963>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m                     \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                     \u001b[0merr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8e5a6f0f6963>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0maxis_in_condition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis_in_condition\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWzSNYlIgw4C",
        "outputId": "0507e44f-c234-4dbd-8200-29b1338a9eef"
      },
      "source": [
        "5/8"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPv6S0utr2Bg",
        "outputId": "e0a1f269-f821-4b79-8825-23ede6652287"
      },
      "source": [
        "(2* 0.2 * 0.5)/(0.2+0.5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.28571428571428575"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwpnp5Iysy0G",
        "outputId": "efb8efba-b936-46a4-94f0-3de3de6cc117"
      },
      "source": [
        "1 + ((435-7 + 2 *5) /3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "147.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc7rAqVyyQLA",
        "outputId": "bf6c88a3-9b5d-4171-9126-541f9b5662ca"
      },
      "source": [
        "1 + (140/2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "71.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUsgPOv-ywB5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}